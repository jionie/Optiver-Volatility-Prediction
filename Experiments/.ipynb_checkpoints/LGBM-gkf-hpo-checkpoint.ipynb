{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import pandas as pd\n",
    "from pandas.core.common import flatten\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import lightgbm as lgb\n",
    "from flaml import tune\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"max_columns\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"root_dir\": \"../input/optiver-realized-volatility-prediction/\",\n",
    "    \"ckpt_dir\": \"../ckpts\",\n",
    "    \"kfold_seed\": 42,\n",
    "    \"n_splits\": 5,\n",
    "    \"n_clusters\": 7,\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "        \"objective\": \"rmse\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"max_depth\": -1, # 4 to 20\n",
    "        \"max_bin\":318, # 40 to 400\n",
    "        \"min_data_in_leaf\":2000, # 100 to 2000\n",
    "        # \"n_estimators\":100, # 40 to 400\n",
    "        # \"num_leaves\":31, # 8 to 64\n",
    "        # \"min_child_samples\":20, # 10 to 50\n",
    "        \"learning_rate\": 0.01, # 0.01 to 0.1\n",
    "        \"subsample\": 0.66, # 0.3 to 0.9\n",
    "        # \"log_max_bin\": 8, # 3 to 11\n",
    "        # \"colsample_bytree\": 1.0, # 0.1 to 1.0\n",
    "        # \"reg_alpha\": 0.001, # 0 to 0.1\n",
    "        # \"reg_lambda\": 1.0, # 0.8 to 1.0\n",
    "        \"subsample_freq\": 3, # 0 to 30\n",
    "        \"feature_fraction\": 0.567, # 0.5 to 0.9\n",
    "        \"lambda_l1\": 1.27, # 0.0 to 3.0\n",
    "        \"lambda_l2\": 0.475, # 0.0 to 3.0\n",
    "        \"categorical_column\":[0],\n",
    "        \"seed\":2021,\n",
    "        \"feature_fraction_seed\": 2021,\n",
    "        \"bagging_seed\": 2021,\n",
    "        \"drop_seed\": 2021,\n",
    "        \"data_random_seed\": 2021,\n",
    "        \"n_jobs\":-1,\n",
    "        \"verbose\": -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test():\n",
    "    \n",
    "    train = pd.read_csv(\"../input/optiver-realized-volatility-prediction/train.csv\")\n",
    "    test = pd.read_csv(\"../input/optiver-realized-volatility-prediction/test.csv\")\n",
    "    \n",
    "    # Create a key to merge with book and trade data\n",
    "    train[\"row_id\"] = train[\"stock_id\"].astype(str) + \"-\" + train[\"time_id\"].astype(str)\n",
    "    test[\"row_id\"] = test[\"stock_id\"].astype(str) + \"-\" + test[\"time_id\"].astype(str)\n",
    "    \n",
    "    print(\"Our training set has {} rows\".format(train.shape[0]))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_counts(df):\n",
    "    activity_counts_ = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].agg(\"count\").reset_index()\n",
    "    activity_counts_ = activity_counts_.rename(columns={\"seconds_in_bucket\": \"activity_counts\"})\n",
    "    return activity_counts_\n",
    "\n",
    "\n",
    "def calc_wap(df, pos=1):\n",
    "    wap = (df[\"bid_price{}\".format(pos)] * df[\"ask_size{}\".format(pos)] + df[\"ask_price{}\".format(pos)] * df[\n",
    "        \"bid_size{}\".format(pos)]) / (df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df, pos=1):\n",
    "    wap = (df[\"bid_price{}\".format(pos)] * df[\"bid_size{}\".format(pos)] + df[\"ask_price{}\".format(pos)] * df[\n",
    "        \"ask_size{}\".format(pos)]) / (df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def wp(df):\n",
    "    wp_ = (df[\"bid_price1\"] * df[\"bid_size1\"] + df[\"ask_price1\"] * df[\"ask_size1\"] + df[\"bid_price2\"] * df[\n",
    "        \"bid_size2\"] + df[\"ask_price2\"] * df[\"ask_size2\"]) / (\n",
    "                  df[\"bid_size1\"] + df[\"ask_size1\"] + df[\"bid_size2\"] + df[\"ask_size2\"])\n",
    "    return wp_\n",
    "\n",
    "\n",
    "def maximum_drawdown(series, window=600):\n",
    "    # window for 10 minutes, use min_periods=1 if you want to allow the expanding window\n",
    "    roll_max = series.rolling(window, min_periods=1).max()\n",
    "    second_drawdown = series / roll_max - 1.0\n",
    "    max_drawdown = second_drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff().fillna(0)\n",
    "\n",
    "\n",
    "def rolling_log_return(series, rolling=60):\n",
    "    return np.log(series.rolling(rolling)).diff().fillna(0)\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series ** 2))\n",
    "\n",
    "\n",
    "def diff(series):\n",
    "    return series.diff().fillna(0)\n",
    "\n",
    "\n",
    "def time_diff(series):\n",
    "    return series.diff().fillna(series)\n",
    "\n",
    "\n",
    "def order_flow_imbalance(df, pos=1):\n",
    "    df[\"bid_price{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"bid_price{}\".format(pos)].apply(diff)\n",
    "    df[\"bid_size{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"bid_price{}\".format(pos)].apply(diff)\n",
    "    df[\"bid_order_flow{}\".format(pos)] = df[\"bid_size{}\".format(pos)].copy(deep=True)\n",
    "    df[\"bid_order_flow{}\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] < 0] *= -1\n",
    "    df[\"bid_order_flow{}\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] == 0] = \\\n",
    "        df[\"bid_size{}_diff\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] == 0]\n",
    "\n",
    "    df[\"ask_price{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"ask_price{}\".format(pos)].apply(diff)\n",
    "    df[\"ask_size{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"ask_price{}\".format(pos)].apply(diff)\n",
    "    df[\"ask_order_flow{}\".format(pos)] = df[\"ask_size{}\".format(pos)].copy(deep=True)\n",
    "    df[\"ask_order_flow{}\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] < 0] *= -1\n",
    "    df[\"ask_order_flow{}\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] == 0] = \\\n",
    "        df[\"ask_size{}_diff\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] == 0]\n",
    "\n",
    "    order_flow_imbalance_ = df[\"bid_order_flow{}\".format(pos)] - df[\"ask_order_flow{}\".format(pos)]\n",
    "\n",
    "    df.drop([\"bid_price{}_diff\".format(pos), \"bid_size{}_diff\".format(pos), \"bid_order_flow{}\".format(pos),\n",
    "             \"ask_price{}_diff\".format(pos), \"ask_size{}_diff\".format(pos), \"ask_order_flow{}\".format(pos)], axis=1,\n",
    "            inplace=True)\n",
    "\n",
    "    return order_flow_imbalance_ + 1e-8\n",
    "\n",
    "\n",
    "def order_book_slope(df):\n",
    "\n",
    "    df[\"mid_point\"] = (df[\"bid_price1\"] + df[\"ask_price1\"]) / 2\n",
    "    best_mid_point_ = df.groupby([\"time_id\"])[\"mid_point\"].agg(\"max\").reset_index()\n",
    "    best_mid_point_ = best_mid_point_.rename(columns={\"mid_point\": \"best_mid_point\"})\n",
    "    df = df.merge(best_mid_point_, how=\"left\", on=\"time_id\")\n",
    "\n",
    "    best_mid_point = df[\"best_mid_point\"].copy()\n",
    "    df.drop([\"mid_point\", \"best_mid_point\"], axis=1, inplace=True)\n",
    "\n",
    "    def ratio(series):\n",
    "        ratio_ = series / series.shift()\n",
    "        return ratio_\n",
    "\n",
    "    bid_price1_ratio = df.groupby([\"time_id\"])[\"bid_price1\"].apply(ratio)\n",
    "    bid_price1_mid_point_ratio = df[\"bid_price1\"] / best_mid_point\n",
    "    bid_price1_ratio = abs(bid_price1_ratio.fillna(bid_price1_mid_point_ratio) - 1)\n",
    "\n",
    "    bid_size1_ratio = df.groupby([\"time_id\"])[\"bid_size1\"].apply(ratio) - 1\n",
    "    bid_size1_ratio = bid_size1_ratio.fillna(df[\"bid_size1\"])\n",
    "    df[\"DE\"] = (bid_size1_ratio / bid_price1_ratio).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    ask_price1_ratio = df.groupby([\"time_id\"])[\"ask_price1\"].apply(ratio)\n",
    "    ask_price1_mid_point_ratio = df[\"ask_price1\"] / best_mid_point\n",
    "    ask_price1_ratio = abs(ask_price1_ratio.fillna(ask_price1_mid_point_ratio) - 1)\n",
    "\n",
    "    ask_size1_ratio = df.groupby([\"time_id\"])[\"ask_size1\"].apply(ratio) - 1\n",
    "    ask_size1_ratio = ask_size1_ratio.fillna(df[\"ask_size1\"])\n",
    "    df[\"SE\"] = (ask_size1_ratio / ask_price1_ratio).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    df[\"order_book_slope\"] = (df[\"DE\"] + df[\"SE\"]) / 2\n",
    "    order_book_slope_ = df.groupby([\"time_id\"])[\"order_book_slope\"].agg(\"mean\").reset_index()\n",
    "    df.drop([\"order_book_slope\", \"DE\", \"SE\"], axis=1, inplace=True)\n",
    "\n",
    "    return order_book_slope_\n",
    "\n",
    "\n",
    "def ldispersion(df):\n",
    "    LDispersion = 1 / 2 * (\n",
    "            df[\"bid_size1\"] / (df[\"bid_size1\"] + df[\"bid_size2\"]) * abs(df[\"bid_price1\"] - df[\"bid_price2\"]) + df[\n",
    "        \"ask_size1\"] / (df[\"ask_size1\"] + df[\"ask_size2\"]) * abs(df[\"ask_price1\"] - df[\"ask_price2\"]))\n",
    "    return LDispersion\n",
    "\n",
    "\n",
    "def depth_imbalance(df, pos=1):\n",
    "    depth_imbalance_ = (df[\"bid_size{}\".format(pos)] - df[\"ask_size{}\".format(pos)]) / (\n",
    "            df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "\n",
    "    return depth_imbalance_\n",
    "\n",
    "\n",
    "def height_imbalance(df, pos=1):\n",
    "    height_imbalance_ = (df[\"bid_price{}\".format(pos)] - df[\"ask_price{}\".format(pos)]) / (\n",
    "            df[\"bid_price{}\".format(pos)] + df[\"ask_price{}\".format(pos)])\n",
    "\n",
    "    return height_imbalance_\n",
    "\n",
    "\n",
    "def pressure_imbalance(df):\n",
    "    mid_price = (df[\"bid_price1\"] + df[\"ask_price1\"]) / 2\n",
    "\n",
    "    weight_buy = mid_price / (mid_price - df[\"bid_price1\"]) + mid_price / (mid_price - df[\"bid_price2\"])\n",
    "    pressure_buy = df[\"bid_size1\"] * (mid_price / (mid_price - df[\"bid_price1\"])) / weight_buy + df[\"bid_size2\"] * (\n",
    "            mid_price / (mid_price - df[\"bid_price2\"])) / weight_buy\n",
    "\n",
    "    weight_sell = mid_price / (df[\"ask_price1\"] - mid_price) + mid_price / (df[\"ask_price2\"] - mid_price)\n",
    "    pressure_sell = df[\"ask_size1\"] * (mid_price / (df[\"ask_price1\"] - mid_price)) / weight_sell + df[\"ask_size2\"] * (\n",
    "            mid_price / (df[\"ask_price2\"] - mid_price)) / weight_sell\n",
    "\n",
    "    pressure_imbalance_ = np.log(pressure_buy) - np.log(pressure_sell)\n",
    "\n",
    "    return pressure_imbalance_\n",
    "\n",
    "\n",
    "def relative_spread(df, pos=1):\n",
    "    relative_spread_ = 2 * (df[\"ask_price{}\".format(pos)] - df[\"bid_price{}\".format(pos)]) / (\n",
    "            df[\"ask_price{}\".format(pos)] + df[\"bid_price{}\".format(pos)])\n",
    "\n",
    "    return relative_spread_\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_seconds(df):\n",
    "    \n",
    "    time_ids = list(OrderedDict.fromkeys(df[\"time_id\"]))\n",
    "    \n",
    "    filled_df = pd.DataFrame(index=range(len(time_ids) * 600), columns=df.columns)\n",
    "    \n",
    "    all_seconds_in_bucket = list(flatten([range(600) for i in range(len(time_ids))]))\n",
    "    filled_df[\"seconds_in_bucket\"] = all_seconds_in_bucket\n",
    "    \n",
    "    all_time_ids = list(flatten([[time_ids[i]] * 600 for i in range(len(time_ids))]))\n",
    "    filled_df[\"time_id\"] = all_time_ids\n",
    "    \n",
    "    filled_df = pd.merge(filled_df, df, on=[\"time_id\", \"seconds_in_bucket\"], how=\"left\", suffixes=(\"_to_move\", \"\"))\n",
    "    \n",
    "    to_remove_columns = [column for column in filled_df.columns if \"to_move\" in column]\n",
    "    filled_df = filled_df.drop(to_remove_columns, axis=1)\n",
    "    \n",
    "    filled_df = filled_df.fillna(method=\"ffill\")\n",
    "    \n",
    "    return filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # float 64 to float 32\n",
    "    float_cols = df.select_dtypes(include=[np.float64]).columns\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "    # int 64 to int 32\n",
    "    int_cols = df.select_dtypes(include=[np.int64]).columns\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    # rebase seconds_in_bucket\n",
    "    df[\"seconds_in_bucket\"] = df[\"seconds_in_bucket\"] - df[\"seconds_in_bucket\"].min()\n",
    "\n",
    "    # Calculate seconds gap\n",
    "    df[\"seconds_gap\"] = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].apply(time_diff)\n",
    "\n",
    "    # Calculate Wap\n",
    "    df[\"wap1\"] = calc_wap(df, pos=1)\n",
    "    df[\"wap2\"] = calc_wap(df, pos=2)\n",
    "\n",
    "    # Calculate wap balance\n",
    "    df[\"wap_balance\"] = abs(df[\"wap1\"] - df[\"wap2\"])\n",
    "\n",
    "    # Calculate log returns\n",
    "    df[\"log_return1\"] = df.groupby([\"time_id\"])[\"wap1\"].apply(log_return)\n",
    "    df[\"log_return2\"] = df.groupby([\"time_id\"])[\"wap2\"].apply(log_return)\n",
    "\n",
    "    # Calculate spread\n",
    "    df[\"bid_ask_spread1\"] = df[\"ask_price1\"] / df[\"bid_price1\"] - 1\n",
    "    df[\"bid_ask_spread2\"] = df[\"ask_price2\"] / df[\"bid_price2\"] - 1\n",
    "\n",
    "    # order flow imbalance\n",
    "    df[\"order_flow_imbalance1\"] = order_flow_imbalance(df, 1)\n",
    "    df[\"order_flow_imbalance2\"] = order_flow_imbalance(df, 2)\n",
    "\n",
    "    # order book slope\n",
    "    order_slope_ = order_book_slope(df)\n",
    "    df = df.merge(order_slope_, how=\"left\", on=\"time_id\")\n",
    "\n",
    "    # depth imbalance\n",
    "    df[\"depth_imbalance1\"] = depth_imbalance(df, pos=1)\n",
    "    df[\"depth_imbalance2\"] = depth_imbalance(df, pos=2)\n",
    "\n",
    "    # height imbalance\n",
    "    df[\"height_imbalance1\"] = height_imbalance(df, pos=1)\n",
    "    df[\"height_imbalance2\"] = height_imbalance(df, pos=2)\n",
    "\n",
    "    # pressure imbalance\n",
    "    df[\"pressure_imbalance\"] = pressure_imbalance(df)\n",
    "\n",
    "    # total volume\n",
    "    df[\"total_volume\"] = (df[\"ask_size1\"] + df[\"ask_size2\"]) + (df[\"bid_size1\"] + df[\"bid_size2\"])\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        \"wap1\": [np.sum, np.std],\n",
    "        \"wap2\": [np.sum, np.std],\n",
    "        \"log_return1\": [realized_volatility],\n",
    "        \"log_return2\": [realized_volatility],\n",
    "        \"wap_balance\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_book_slope\": [np.mean, np.max],\n",
    "        \"depth_imbalance1\": [np.sum, np.max, np.std],\n",
    "        \"depth_imbalance2\": [np.sum, np.max, np.std],\n",
    "        \"height_imbalance1\": [np.sum, np.max, np.std],\n",
    "        \"height_imbalance2\": [np.sum, np.max, np.std],\n",
    "        \"pressure_imbalance\": [np.sum, np.max, np.std],\n",
    "        \"total_volume\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        \"log_return1\": [realized_volatility],\n",
    "        \"log_return2\": [realized_volatility],\n",
    "        \"wap_balance\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"total_volume\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(feature_dict, seconds_in_bucket, add_suffix=False):\n",
    "        # Group by the window\n",
    "        df_feature_ = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(\n",
    "            feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature_.columns = [\"_\".join(col) for col in df_feature_.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature_ = df_feature_.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "        return df_feature_\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    windows = [0, 150, 300, 450]\n",
    "    add_suffixes = [False, True, True, True]\n",
    "    df_feature = None\n",
    "\n",
    "    for window, add_suffix in zip(windows, add_suffixes):\n",
    "        if df_feature is None:\n",
    "            df_feature = get_stats_window(feature_dict=create_feature_dict, seconds_in_bucket=window,\n",
    "                                          add_suffix=add_suffix)\n",
    "        else:\n",
    "            new_df_feature = get_stats_window(feature_dict=create_feature_dict_time, seconds_in_bucket=window,\n",
    "                                              add_suffix=add_suffix)\n",
    "            df_feature = df_feature.merge(new_df_feature, how=\"left\", left_on=\"time_id_\",\n",
    "                                          right_on=\"time_id__{}\".format(window))\n",
    "\n",
    "            # Drop unnecesary time_ids\n",
    "            df_feature.drop([\"time_id__{}\".format(window)], axis=1, inplace=True)\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature.drop([\"time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # float 64 to float 32\n",
    "    float_cols = df.select_dtypes(include=[np.float64]).columns\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "    # int 64 to int 32\n",
    "    int_cols = df.select_dtypes(include=[np.int64]).columns\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    # rebase seconds_in_bucket\n",
    "    df[\"seconds_in_bucket\"] = df[\"seconds_in_bucket\"] - df[\"seconds_in_bucket\"].min()\n",
    "\n",
    "    # Calculate seconds gap\n",
    "    df[\"seconds_gap\"] = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].apply(time_diff)\n",
    "\n",
    "    # Calculate log return\n",
    "    df[\"price_log_return\"] = df.groupby(\"time_id\")[\"price\"].apply(log_return)\n",
    "\n",
    "    # Calculate volumes\n",
    "    df[\"volumes\"] = df[\"price\"] * df[\"size\"]\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        \"price_log_return\": [realized_volatility],\n",
    "        \"volumes\": [np.sum, np.max, np.std],\n",
    "        \"order_count\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        \"price_log_return\": [realized_volatility],\n",
    "        \"volumes\": [np.sum, np.max, np.std],\n",
    "        \"order_count\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(feature_dict, seconds_in_bucket, add_suffix=False):\n",
    "        # Group by the window\n",
    "        df_feature_ = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(\n",
    "            feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature_.columns = [\"_\".join(col) for col in df_feature_.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature_ = df_feature_.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "        return df_feature_\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    windows = [0, 150, 300, 450]\n",
    "    add_suffixes = [False, True, True, True]\n",
    "    df_feature = None\n",
    "\n",
    "    for window, add_suffix in zip(windows, add_suffixes):\n",
    "        if df_feature is None:\n",
    "            df_feature = get_stats_window(feature_dict=create_feature_dict, seconds_in_bucket=window,\n",
    "                                          add_suffix=add_suffix)\n",
    "        else:\n",
    "            new_df_feature = get_stats_window(feature_dict=create_feature_dict_time, seconds_in_bucket=window,\n",
    "                                              add_suffix=add_suffix)\n",
    "            df_feature = df_feature.merge(new_df_feature, how=\"left\", left_on=\"time_id_\",\n",
    "                                          right_on=\"time_id__{}\".format(window))\n",
    "\n",
    "            # Drop unnecesary time_ids\n",
    "            df_feature.drop([\"time_id__{}\".format(window)], axis=1, inplace=True)\n",
    "\n",
    "    def tendency(price, vol):\n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff / price[1:]) * 100\n",
    "        power = np.sum(val * vol[1:])\n",
    "        return (power)\n",
    "\n",
    "    lis = []\n",
    "    for n_time_id in df[\"time_id\"].unique():\n",
    "        \n",
    "        df_id = df[df[\"time_id\"] == n_time_id]\n",
    "        \n",
    "        tendencyV = tendency(df_id[\"price\"].values, df_id[\"size\"].values)\n",
    "        energy = np.mean(df_id[\"price\"].values ** 2)\n",
    "\n",
    "        lis.append(\n",
    "            {\n",
    "                \"time_id\": n_time_id,\n",
    "                \"tendency\": tendencyV,\n",
    "                \"energy\": energy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_feature = df_feature.merge(df_lr, how=\"left\", left_on=\"time_id_\", right_on=\"time_id\")\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    df_feature = df_feature.add_prefix(\"trade_\")\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"trade_time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature.drop([\"trade_time_id_\", \"trade_time_id\"], axis=1, inplace=True)\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = CONFIG[\"root_dir\"] + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = CONFIG[\"root_dir\"] + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = CONFIG[\"root_dir\"] + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = CONFIG[\"root_dir\"] + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = \"row_id\", how = \"left\")\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    \n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  3.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 56 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train[\"stock_id\"].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train=True)\n",
    "train = train.merge(train_, on=[\"row_id\"], how=\"left\")\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test[\"stock_id\"].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train=False)\n",
    "test = test.merge(test_, on=[\"row_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs log columns\n",
    "abs_log_columns = [column for column in train.columns if \n",
    "                       \"order_flow_imbalance\" in column or \n",
    "                       \"order_book_slope\" in column or \n",
    "                       \"depth_imbalance\" in column or \n",
    "                       \"pressure_imbalance\" in column or\n",
    "                       \"total_volume\" in column or\n",
    "                       \"seconds_gap\" in column or\n",
    "                       \"trade_volumes\" in column or\n",
    "                       \"trade_order_count\" in column or\n",
    "                       \"trade_seconds_gap\" in column or\n",
    "                       \"trade_tendency\" in column\n",
    "                      ]\n",
    "\n",
    "# apply abs + log1p\n",
    "train[abs_log_columns] = (train[abs_log_columns].apply(np.abs)).apply(np.log1p)\n",
    "test[abs_log_columns] = (test[abs_log_columns].apply(np.abs)).apply(np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "train = train.fillna(train.mean())\n",
    "test = test.fillna(train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process agg by kmeans\n",
    "def get_kmeans_idx(n_clusters=7):\n",
    "    train_p = pd.read_csv(\"../input/optiver-realized-volatility-prediction/train.csv\")\n",
    "    train_p = train_p.pivot(index=\"time_id\", columns=\"stock_id\", values=\"target\")\n",
    "\n",
    "    corr = train_p.corr()\n",
    "\n",
    "    ids = corr.index\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(corr.values)\n",
    "\n",
    "    kmeans_clusters = []\n",
    "    for n in range(n_clusters):\n",
    "        kmeans_clusters.append ([(x - 1) for x in ((ids + 1)*(kmeans.labels_ == n)) if x > 0])\n",
    "        \n",
    "    return kmeans_clusters\n",
    "    \n",
    "\n",
    "def agg_stat_features_by_clusters(df, n_clusters=7, function=np.nanmean, post_fix=\"_cluster_mean\"):\n",
    "\n",
    "    kmeans_clusters = get_kmeans_idx(n_clusters=n_clusters)\n",
    "\n",
    "    clusters = []\n",
    "    agg_columns = [\n",
    "        \"time_id\",\n",
    "        \"stock_id\",\n",
    "        \"log_return1_realized_volatility\",\n",
    "        \"log_return2_realized_volatility\",\n",
    "        \"order_flow_imbalance1_sum\",\n",
    "        \"order_flow_imbalance2_sum\",\n",
    "        \"order_book_slope_mean\",\n",
    "        \"depth_imbalance1_std\",\n",
    "        \"depth_imbalance2_std\",\n",
    "        \"height_imbalance1_sum\",\n",
    "        \"height_imbalance2_sum\",\n",
    "        \"pressure_imbalance_std\",\n",
    "        \"total_volume_sum\",\n",
    "        \"seconds_gap_mean\",\n",
    "        \"trade_price_log_return_realized_volatility\",\n",
    "        \"trade_volumes_sum\",\n",
    "        \"trade_order_count_sum\",\n",
    "        \"trade_seconds_gap_mean\",\n",
    "        \"trade_tendency\",\n",
    "        \"trade_energy\"\n",
    "    ]\n",
    "\n",
    "    for cluster_idx, ind in enumerate(kmeans_clusters):\n",
    "        cluster_df = df.loc[df[\"stock_id\"].isin(ind), agg_columns].groupby([\"time_id\"]).agg(function)\n",
    "        cluster_df.loc[:, \"stock_id\"] = str(cluster_idx) + post_fix\n",
    "        clusters.append(cluster_df)\n",
    "\n",
    "    clusters_df = pd.concat(clusters).reset_index()\n",
    "    # multi index (column, c1)\n",
    "    clusters_df = clusters_df.pivot(index=\"time_id\", columns=\"stock_id\")\n",
    "    # ravel multi index to list of tuple [(target, c1), ...]\n",
    "    clusters_df.columns = [\"_\".join(x) for x in clusters_df.columns.ravel()]\n",
    "    clusters_df.reset_index(inplace=True)\n",
    "\n",
    "    postfixes = [\n",
    "        \"0\" + post_fix,\n",
    "        \"1\" + post_fix,\n",
    "        \"3\" + post_fix,\n",
    "        \"4\" + post_fix,\n",
    "        \"6\" + post_fix,\n",
    "    ]\n",
    "    merge_columns = []\n",
    "    for column in agg_columns:\n",
    "        if column == \"time_id\":\n",
    "            merge_columns.append(column)\n",
    "        elif column == \"stock_id\":\n",
    "            continue\n",
    "        else:\n",
    "            for postfix in postfixes:\n",
    "                merge_columns.append(column + \"_\" + postfix)\n",
    "                \n",
    "    not_exist_columns = [column for column in merge_columns if column not in clusters_df.columns]\n",
    "    clusters_df[not_exist_columns] = 0\n",
    "    \n",
    "    df = pd.merge(df, clusters_df[merge_columns], how=\"left\", on=\"time_id\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to get group stats for the time_id\n",
    "def agg_stat_features_by_market(df, operations=None, operations_names=None):\n",
    "    def percentile(n):\n",
    "        def percentile_(x):\n",
    "            return np.percentile(x, n)\n",
    "\n",
    "        percentile_.__name__ = \"percentile_%s\" % n\n",
    "        return percentile_\n",
    "\n",
    "    if operations is None:\n",
    "        operations = [\n",
    "            np.nanmean,\n",
    "        ]\n",
    "        operations_names = [\n",
    "            \"mean\",\n",
    "        ]\n",
    "\n",
    "    # Get realized volatility columns\n",
    "    vol_cols = [\n",
    "        \"log_return1_realized_volatility\",\n",
    "        \"log_return1_realized_volatility_150\",\n",
    "        \"log_return1_realized_volatility_300\",\n",
    "        \"log_return1_realized_volatility_450\",\n",
    "    ]\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby([\"stock_id\"])[vol_cols].agg(operations).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = [\"_\".join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix(\"_\" + \"stock\")\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby([\"time_id\"])[vol_cols].agg(operations).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = [\"_\".join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix(\"_\" + \"time\")\n",
    "\n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how=\"left\", left_on=[\"stock_id\"], right_on=[\"stock_id__stock\"])\n",
    "    df.drop(\"stock_id__stock\", axis=1, inplace=True)\n",
    "\n",
    "    df = df.merge(df_time_id, how=\"left\", left_on=[\"time_id\"], right_on=[\"time_id__time\"])\n",
    "    df.drop(\"time_id__time\", axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return \"RMSPE\", rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate(train, test,\n",
    "                    #    max_depth,\n",
    "                       max_bin,\n",
    "                       min_data_in_leaf,\n",
    "                    #    n_estimators,\n",
    "                    #    num_leaves,\n",
    "                    #    min_child_samples,\n",
    "                       learning_rate,\n",
    "                       subsample,\n",
    "                    #    log_max_bin,\n",
    "                    #    colsample_bytree,\n",
    "                    #    reg_alpha,\n",
    "                    #    reg_lambda,\n",
    "                       subsample_freq,\n",
    "                       feature_fraction,\n",
    "                       lambda_l1,\n",
    "                       lambda_l2):\n",
    "    \n",
    "    # PARAMS['max_depth'] = max_depth\n",
    "    PARAMS['max_bin'] = max_bin\n",
    "    PARAMS['min_data_in_leaf'] = min_data_in_leaf\n",
    "    # PARAMS['n_estimators'] = n_estimators\n",
    "    # PARAMS['num_leaves'] = num_leaves\n",
    "    # PARAMS['min_child_samples'] = min_child_samples\n",
    "    PARAMS['learning_rate'] = learning_rate\n",
    "    PARAMS['subsample'] = subsample\n",
    "    # PARAMS['log_max_bin'] = log_max_bin\n",
    "    # PARAMS['colsample_bytree'] = colsample_bytree\n",
    "    # PARAMS['reg_alpha'] = reg_alpha\n",
    "    # PARAMS['reg_lambda'] = reg_lambda\n",
    "    PARAMS['subsample_freq'] = subsample_freq\n",
    "    PARAMS['feature_fraction'] = feature_fraction\n",
    "    PARAMS['lambda_l1'] = lambda_l1\n",
    "    PARAMS['lambda_l2'] = lambda_l2\n",
    "    # scale\n",
    "    # scaler = QuantileTransformer(n_quantiles=2000, random_state=2021)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Split features and target\n",
    "    x = train.drop([\"row_id\", \"target\"], axis=1)\n",
    "    y = train[\"target\"]\n",
    "    \n",
    "    # x_test with train feature\n",
    "    x_test = test.drop(\"row_id\", axis=1)\n",
    "    x_test = agg_stat_features_by_market(x_test)\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "    \n",
    "    # define normalize columns\n",
    "    except_columns = [\"stock_id\", \"time_id\", \"target\", \"row_id\"]\n",
    "    normalized_columns = [column for column in x_test.columns if column not in except_columns]\n",
    "    x_test.drop(\"time_id\", axis=1, inplace=True)\n",
    "    \n",
    "    # Transform stock id to a numeric value\n",
    "    x[\"stock_id\"] = x[\"stock_id\"].astype(int)\n",
    "    x_test[\"stock_id\"] = x_test[\"stock_id\"].astype(int)\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    \n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    \n",
    "    # Create a KFold object\n",
    "    kfold = GroupKFold(n_splits=CONFIG[\"n_splits\"])\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x, groups=x[\"time_id\"])):\n",
    "        \n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        x_train = x.iloc[trn_ind]\n",
    "        x_train = agg_stat_features_by_market(x_train)\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "        x_train.drop(\"time_id\", axis=1, inplace=True)\n",
    "        scaler = scaler.fit(x_train[normalized_columns])\n",
    "        # dump(scaler, os.path.join(CONFIG[\"ckpt_dir\"], \"std_scaler_fold_{}.bin\".format(fold + 1)), compress=True)\n",
    "        x_train[normalized_columns] = scaler.transform(x_train[normalized_columns])\n",
    "        \n",
    "        x_val = x.iloc[val_ind]\n",
    "        x_val = agg_stat_features_by_market(x_val)\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "        x_val.drop(\"time_id\", axis=1, inplace=True)\n",
    "        x_val[normalized_columns] = scaler.transform(x_val[normalized_columns])\n",
    "        \n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        \n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = [\"stock_id\"])\n",
    "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = [\"stock_id\"])\n",
    "        \n",
    "        # Train\n",
    "        model = lgb.train(params = PARAMS, \n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          num_boost_round = 8000, \n",
    "                          early_stopping_rounds = 400, \n",
    "                          verbose_eval = 200,\n",
    "                          feval = feval_rmspe\n",
    "                         )\n",
    "        \n",
    "        # model.save_model(os.path.join(CONFIG[\"ckpt_dir\"], \"lgbm_fold_{}.txt\".format(fold + 1)))\n",
    "        \n",
    "        # # Feature Importance\n",
    "        # fig, ax = plt.subplots(figsize=(12, 30))\n",
    "        # for feature, importance in zip(model.feature_name(), model.feature_importance()):\n",
    "        #     print(feature, importance)\n",
    "        # lgb.plot_importance(model, max_num_features=50, ax=ax)\n",
    "        # plt.title(\"Feature importance\")\n",
    "        # plt.show()\n",
    "        \n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val)\n",
    "        \n",
    "        # Predict the test set\n",
    "        x_test_ = x_test.copy()\n",
    "        x_test_[normalized_columns] = scaler.transform(x_test_[normalized_columns])\n",
    "        test_predictions += model.predict(x_test_) / CONFIG[\"n_splits\"]\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f\"Our out of folds RMSPE is {rmspe_score}\")\n",
    "    \n",
    "    # Return test predictions\n",
    "    return rmspe_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predictions = train_and_evaluate(train, test)\n",
    "\n",
    "def evaluate_config(config):\n",
    "    metric = train_and_evaluate(train, test, \n",
    "                                # config['max_depth'],\n",
    "                                config['max_bin'],\n",
    "                                config['min_data_in_leaf'],\n",
    "                                # config['n_estimators'],\n",
    "                                # config['num_leaves'],\n",
    "                                # config['min_child_samples'],\n",
    "                                config['learning_rate'],\n",
    "                                config['subsample'],\n",
    "                                # config['log_max_bin'],\n",
    "                                # config['colsample_bytree'],\n",
    "                                # config['reg_alpha'],\n",
    "                                # config['reg_lambda'],\n",
    "                                config['subsample_freq'],\n",
    "                                config['feature_fraction'],\n",
    "                                config['lambda_l1'],\n",
    "                                config['lambda_l2'])  \n",
    "    tune.report(metric=metric) \n",
    "\n",
    "analysis = tune.run(\n",
    "    evaluate_config,    # the function to evaluate a config\n",
    "    config={\n",
    "        # 'max_depth': tune.randint(lower=4, upper=20),\n",
    "        'max_bin': tune.randint(lower=40, upper=400),\n",
    "        'min_data_in_leaf': tune.randint(lower=100, upper=2000),\n",
    "        # 'n_estimators': tune.randint(lower=40, upper=400),\n",
    "        # 'num_leaves': tune.randint(lower=8, upper=64),\n",
    "        # 'min_child_samples': tune.randint(lower=10, upper=50),\n",
    "        'learning_rate': tune.loguniform(lower=0.01, upper=0.1),\n",
    "        'subsample': tune.uniform(lower=0.3, upper=0.9),\n",
    "        # 'log_max_bin': tune.randint(lower=3, upper=11),\n",
    "        # 'colsample_bytree': tune.uniform(lower=0.8, upper=1.0),\n",
    "        # 'reg_alpha': tune.uniform(lower=0, upper=0.1),\n",
    "        # 'reg_lambda': tune.uniform(lower=0.8, upper=1.0),\n",
    "        'subsample_freq': tune.randint(lower=0, upper=30),\n",
    "        'feature_fraction': tune.uniform(lower=0.5, upper=0.9),\n",
    "        'lambda_l1': tune.uniform(lower=0.0, upper=3.0),\n",
    "        'lambda_l2': tune.uniform(lower=0.0, upper=3.0),\n",
    "    }, # the search space\n",
    "    low_cost_partial_config={\n",
    "        # \"max_depth\": 10, # 4 to 20\n",
    "        \"max_bin\":100, # 40 to 400\n",
    "        \"min_data_in_leaf\":500, # 100 to 2000\n",
    "        # \"n_estimators\":100, # 8 to 100\n",
    "        # \"num_leaves\":31, # 8 to 64\n",
    "        # \"min_child_samples\":20, # 10 to 50\n",
    "        \"learning_rate\": 0.05, # 0.01 to 0.1\n",
    "        \"subsample\": 0.72, # 0.3 to 0.9\n",
    "        # \"log_max_bin\": 8, # 3 to 11\n",
    "        # \"colsample_bytree\": 1.0, # 0.1 to 1.0\n",
    "        # \"reg_alpha\": 0.001, # 0 to 0.1\n",
    "        # \"reg_lambda\": 1.0, # 0.8 to 1.0\n",
    "        \"subsample_freq\": 4, # 0 to 30\n",
    "        \"feature_fraction\": 0.5, # 0.5 to 0.9\n",
    "        \"lambda_l1\": 0.5, # 0 to 3.0\n",
    "        \"lambda_l2\": 1.0, # 0 to 3.0\n",
    "        },   \n",
    "    metric='metric',    # the name of the metric used for optimization\n",
    "    mode='min',         # the optimization mode, 'min' or 'max'\n",
    "    num_samples=-1,    # the maximal number of configs to try, -1 means infinite\n",
    "    time_budget_s=60*60*18,   # the time budget in seconds\n",
    "    local_dir='../ckpts/',  # the local directory to store logs\n",
    "    verbose=1,          # verbosity\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metric': 0.21397235975991927, 'training_iteration': 0, 'config': {'max_bin': 318, 'min_data_in_leaf': 1999, 'learning_rate': 0.010963191811858479, 'subsample': 0.66139376817775, 'subsample_freq': 3, 'feature_fraction': 0.5672833917279606, 'lambda_l1': 1.270529105190135, 'lambda_l2': 0.4746440084444883}, 'config/max_bin': 318, 'config/min_data_in_leaf': 1999, 'config/learning_rate': 0.010963191811858479, 'config/subsample': 0.66139376817775, 'config/subsample_freq': 3, 'config/feature_fraction': 0.5672833917279606, 'config/lambda_l1': 1.270529105190135, 'config/lambda_l2': 0.4746440084444883, 'experiment_tag': 'exp', 'time_total_s': 2190.358256340027}\n",
      "{'max_bin': 318, 'min_data_in_leaf': 1999, 'learning_rate': 0.010963191811858479, 'subsample': 0.66139376817775, 'subsample_freq': 3, 'feature_fraction': 0.5672833917279606, 'lambda_l1': 1.270529105190135, 'lambda_l2': 0.4746440084444883}\n"
     ]
    }
   ],
   "source": [
    "print(analysis.best_trial.last_result)  # the best trial's result\n",
    "print(analysis.best_config) # the best config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\ttraining's rmse: 0.000509847\ttraining's RMSPE: 0.236169\tvalid_1's rmse: 0.000527885\tvalid_1's RMSPE: 0.243458\n",
      "[400]\ttraining's rmse: 0.000457427\ttraining's RMSPE: 0.211887\tvalid_1's rmse: 0.000485079\tvalid_1's RMSPE: 0.223716\n",
      "[600]\ttraining's rmse: 0.000441804\ttraining's RMSPE: 0.204651\tvalid_1's rmse: 0.000478393\tvalid_1's RMSPE: 0.220632\n",
      "[800]\ttraining's rmse: 0.000431692\ttraining's RMSPE: 0.199967\tvalid_1's rmse: 0.000475577\tvalid_1's RMSPE: 0.219334\n",
      "[1000]\ttraining's rmse: 0.000424104\ttraining's RMSPE: 0.196452\tvalid_1's rmse: 0.000473453\tvalid_1's RMSPE: 0.218354\n",
      "[1200]\ttraining's rmse: 0.000417791\ttraining's RMSPE: 0.193527\tvalid_1's rmse: 0.000471813\tvalid_1's RMSPE: 0.217598\n",
      "[1400]\ttraining's rmse: 0.000412682\ttraining's RMSPE: 0.191161\tvalid_1's rmse: 0.000470602\tvalid_1's RMSPE: 0.217039\n",
      "[1600]\ttraining's rmse: 0.000408187\ttraining's RMSPE: 0.189079\tvalid_1's rmse: 0.000469592\tvalid_1's RMSPE: 0.216574\n",
      "[1800]\ttraining's rmse: 0.000404369\ttraining's RMSPE: 0.18731\tvalid_1's rmse: 0.000468731\tvalid_1's RMSPE: 0.216176\n",
      "[2000]\ttraining's rmse: 0.00040098\ttraining's RMSPE: 0.18574\tvalid_1's rmse: 0.000468045\tvalid_1's RMSPE: 0.21586\n",
      "[2200]\ttraining's rmse: 0.000397854\ttraining's RMSPE: 0.184293\tvalid_1's rmse: 0.000467423\tvalid_1's RMSPE: 0.215573\n",
      "[2400]\ttraining's rmse: 0.000395117\ttraining's RMSPE: 0.183025\tvalid_1's rmse: 0.000466956\tvalid_1's RMSPE: 0.215358\n",
      "[2600]\ttraining's rmse: 0.000392575\ttraining's RMSPE: 0.181847\tvalid_1's rmse: 0.000466559\tvalid_1's RMSPE: 0.215175\n",
      "[2800]\ttraining's rmse: 0.000390259\ttraining's RMSPE: 0.180775\tvalid_1's rmse: 0.000466275\tvalid_1's RMSPE: 0.215044\n",
      "[3000]\ttraining's rmse: 0.000388008\ttraining's RMSPE: 0.179732\tvalid_1's rmse: 0.000465909\tvalid_1's RMSPE: 0.214875\n",
      "[3200]\ttraining's rmse: 0.000385965\ttraining's RMSPE: 0.178785\tvalid_1's rmse: 0.000465569\tvalid_1's RMSPE: 0.214718\n",
      "[3400]\ttraining's rmse: 0.000384\ttraining's RMSPE: 0.177875\tvalid_1's rmse: 0.000465238\tvalid_1's RMSPE: 0.214565\n",
      "[3600]\ttraining's rmse: 0.000382151\ttraining's RMSPE: 0.177018\tvalid_1's rmse: 0.000465073\tvalid_1's RMSPE: 0.214489\n",
      "[3800]\ttraining's rmse: 0.000380368\ttraining's RMSPE: 0.176193\tvalid_1's rmse: 0.000464919\tvalid_1's RMSPE: 0.214419\n",
      "[4000]\ttraining's rmse: 0.000378674\ttraining's RMSPE: 0.175408\tvalid_1's rmse: 0.000464789\tvalid_1's RMSPE: 0.214359\n",
      "[4200]\ttraining's rmse: 0.000377083\ttraining's RMSPE: 0.174671\tvalid_1's rmse: 0.000464605\tvalid_1's RMSPE: 0.214274\n",
      "[4400]\ttraining's rmse: 0.000375561\ttraining's RMSPE: 0.173966\tvalid_1's rmse: 0.000464383\tvalid_1's RMSPE: 0.214171\n",
      "[4600]\ttraining's rmse: 0.000374075\ttraining's RMSPE: 0.173278\tvalid_1's rmse: 0.000464291\tvalid_1's RMSPE: 0.214129\n",
      "[4800]\ttraining's rmse: 0.00037268\ttraining's RMSPE: 0.172631\tvalid_1's rmse: 0.000464183\tvalid_1's RMSPE: 0.214079\n",
      "[5000]\ttraining's rmse: 0.000371307\ttraining's RMSPE: 0.171995\tvalid_1's rmse: 0.000464044\tvalid_1's RMSPE: 0.214015\n",
      "[5200]\ttraining's rmse: 0.000370008\ttraining's RMSPE: 0.171394\tvalid_1's rmse: 0.00046405\tvalid_1's RMSPE: 0.214018\n",
      "[5400]\ttraining's rmse: 0.000368745\ttraining's RMSPE: 0.170808\tvalid_1's rmse: 0.000463945\tvalid_1's RMSPE: 0.213969\n",
      "[5600]\ttraining's rmse: 0.000367513\ttraining's RMSPE: 0.170238\tvalid_1's rmse: 0.00046384\tvalid_1's RMSPE: 0.213921\n",
      "[5800]\ttraining's rmse: 0.00036632\ttraining's RMSPE: 0.169685\tvalid_1's rmse: 0.000463807\tvalid_1's RMSPE: 0.213906\n",
      "[6000]\ttraining's rmse: 0.000365163\ttraining's RMSPE: 0.16915\tvalid_1's rmse: 0.000463746\tvalid_1's RMSPE: 0.213877\n",
      "[6200]\ttraining's rmse: 0.000364032\ttraining's RMSPE: 0.168626\tvalid_1's rmse: 0.000463727\tvalid_1's RMSPE: 0.213868\n",
      "[6400]\ttraining's rmse: 0.000362921\ttraining's RMSPE: 0.168111\tvalid_1's rmse: 0.000463671\tvalid_1's RMSPE: 0.213843\n",
      "[6600]\ttraining's rmse: 0.000361838\ttraining's RMSPE: 0.167609\tvalid_1's rmse: 0.000463646\tvalid_1's RMSPE: 0.213831\n",
      "[6800]\ttraining's rmse: 0.000360791\ttraining's RMSPE: 0.167124\tvalid_1's rmse: 0.000463663\tvalid_1's RMSPE: 0.213839\n",
      "[7000]\ttraining's rmse: 0.00035977\ttraining's RMSPE: 0.166651\tvalid_1's rmse: 0.000463707\tvalid_1's RMSPE: 0.213859\n",
      "Early stopping, best iteration is:\n",
      "[6631]\ttraining's rmse: 0.000361666\ttraining's RMSPE: 0.16753\tvalid_1's rmse: 0.000463631\tvalid_1's RMSPE: 0.213824\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\ttraining's rmse: 0.00050895\ttraining's RMSPE: 0.235073\tvalid_1's rmse: 0.000543653\tvalid_1's RMSPE: 0.25363\n",
      "[400]\ttraining's rmse: 0.000457055\ttraining's RMSPE: 0.211104\tvalid_1's rmse: 0.000500738\tvalid_1's RMSPE: 0.233609\n",
      "[600]\ttraining's rmse: 0.000441622\ttraining's RMSPE: 0.203976\tvalid_1's rmse: 0.00049284\tvalid_1's RMSPE: 0.229925\n",
      "[800]\ttraining's rmse: 0.000431573\ttraining's RMSPE: 0.199334\tvalid_1's rmse: 0.000489524\tvalid_1's RMSPE: 0.228378\n",
      "[1000]\ttraining's rmse: 0.000423989\ttraining's RMSPE: 0.195831\tvalid_1's rmse: 0.000487597\tvalid_1's RMSPE: 0.227478\n",
      "[1200]\ttraining's rmse: 0.000417906\ttraining's RMSPE: 0.193022\tvalid_1's rmse: 0.000485946\tvalid_1's RMSPE: 0.226708\n",
      "[1400]\ttraining's rmse: 0.000412563\ttraining's RMSPE: 0.190554\tvalid_1's rmse: 0.000484875\tvalid_1's RMSPE: 0.226209\n",
      "[1600]\ttraining's rmse: 0.000408089\ttraining's RMSPE: 0.188488\tvalid_1's rmse: 0.000484076\tvalid_1's RMSPE: 0.225836\n",
      "[1800]\ttraining's rmse: 0.000404285\ttraining's RMSPE: 0.186731\tvalid_1's rmse: 0.000483198\tvalid_1's RMSPE: 0.225426\n",
      "[2000]\ttraining's rmse: 0.000400872\ttraining's RMSPE: 0.185154\tvalid_1's rmse: 0.000482585\tvalid_1's RMSPE: 0.22514\n",
      "[2200]\ttraining's rmse: 0.000397774\ttraining's RMSPE: 0.183723\tvalid_1's rmse: 0.00048224\tvalid_1's RMSPE: 0.224979\n",
      "[2400]\ttraining's rmse: 0.000394986\ttraining's RMSPE: 0.182436\tvalid_1's rmse: 0.000481751\tvalid_1's RMSPE: 0.224751\n",
      "[2600]\ttraining's rmse: 0.000392367\ttraining's RMSPE: 0.181226\tvalid_1's rmse: 0.000481569\tvalid_1's RMSPE: 0.224666\n",
      "[2800]\ttraining's rmse: 0.000389963\ttraining's RMSPE: 0.180116\tvalid_1's rmse: 0.000481491\tvalid_1's RMSPE: 0.22463\n",
      "[3000]\ttraining's rmse: 0.000387691\ttraining's RMSPE: 0.179066\tvalid_1's rmse: 0.00048155\tvalid_1's RMSPE: 0.224657\n",
      "[3200]\ttraining's rmse: 0.00038564\ttraining's RMSPE: 0.178119\tvalid_1's rmse: 0.000481588\tvalid_1's RMSPE: 0.224675\n",
      "Early stopping, best iteration is:\n",
      "[2926]\ttraining's rmse: 0.000388483\ttraining's RMSPE: 0.179432\tvalid_1's rmse: 0.000481291\tvalid_1's RMSPE: 0.224537\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\ttraining's rmse: 0.000514278\ttraining's RMSPE: 0.237682\tvalid_1's rmse: 0.000510441\tvalid_1's RMSPE: 0.237552\n",
      "[400]\ttraining's rmse: 0.000460878\ttraining's RMSPE: 0.213003\tvalid_1's rmse: 0.000466635\tvalid_1's RMSPE: 0.217166\n",
      "[600]\ttraining's rmse: 0.000444519\ttraining's RMSPE: 0.205442\tvalid_1's rmse: 0.000461634\tvalid_1's RMSPE: 0.214838\n",
      "[800]\ttraining's rmse: 0.000434018\ttraining's RMSPE: 0.200589\tvalid_1's rmse: 0.000459806\tvalid_1's RMSPE: 0.213988\n",
      "[1000]\ttraining's rmse: 0.000426195\ttraining's RMSPE: 0.196973\tvalid_1's rmse: 0.00045836\tvalid_1's RMSPE: 0.213315\n",
      "[1200]\ttraining's rmse: 0.000419898\ttraining's RMSPE: 0.194063\tvalid_1's rmse: 0.000457527\tvalid_1's RMSPE: 0.212927\n",
      "[1400]\ttraining's rmse: 0.000414615\ttraining's RMSPE: 0.191621\tvalid_1's rmse: 0.000456862\tvalid_1's RMSPE: 0.212617\n",
      "[1600]\ttraining's rmse: 0.000410034\ttraining's RMSPE: 0.189504\tvalid_1's rmse: 0.000456231\tvalid_1's RMSPE: 0.212324\n",
      "[1800]\ttraining's rmse: 0.000406203\ttraining's RMSPE: 0.187734\tvalid_1's rmse: 0.000455772\tvalid_1's RMSPE: 0.21211\n",
      "[2000]\ttraining's rmse: 0.000402801\ttraining's RMSPE: 0.186161\tvalid_1's rmse: 0.000455352\tvalid_1's RMSPE: 0.211915\n",
      "[2200]\ttraining's rmse: 0.000399673\ttraining's RMSPE: 0.184715\tvalid_1's rmse: 0.000455083\tvalid_1's RMSPE: 0.211789\n",
      "[2400]\ttraining's rmse: 0.000396857\ttraining's RMSPE: 0.183414\tvalid_1's rmse: 0.000455021\tvalid_1's RMSPE: 0.211761\n",
      "[2600]\ttraining's rmse: 0.00039432\ttraining's RMSPE: 0.182241\tvalid_1's rmse: 0.000454876\tvalid_1's RMSPE: 0.211693\n",
      "[2800]\ttraining's rmse: 0.000391944\ttraining's RMSPE: 0.181144\tvalid_1's rmse: 0.000454735\tvalid_1's RMSPE: 0.211628\n",
      "[3000]\ttraining's rmse: 0.000389688\ttraining's RMSPE: 0.180101\tvalid_1's rmse: 0.000454766\tvalid_1's RMSPE: 0.211642\n",
      "[3200]\ttraining's rmse: 0.000387652\ttraining's RMSPE: 0.17916\tvalid_1's rmse: 0.000454782\tvalid_1's RMSPE: 0.21165\n",
      "Early stopping, best iteration is:\n",
      "[2820]\ttraining's rmse: 0.000391711\ttraining's RMSPE: 0.181036\tvalid_1's rmse: 0.000454686\tvalid_1's RMSPE: 0.211605\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\ttraining's rmse: 0.000514081\ttraining's RMSPE: 0.237877\tvalid_1's rmse: 0.000519934\tvalid_1's RMSPE: 0.240822\n",
      "[400]\ttraining's rmse: 0.000461078\ttraining's RMSPE: 0.213351\tvalid_1's rmse: 0.000472044\tvalid_1's RMSPE: 0.21864\n",
      "[600]\ttraining's rmse: 0.000444943\ttraining's RMSPE: 0.205885\tvalid_1's rmse: 0.000464773\tvalid_1's RMSPE: 0.215272\n",
      "[800]\ttraining's rmse: 0.000434308\ttraining's RMSPE: 0.200963\tvalid_1's rmse: 0.000462132\tvalid_1's RMSPE: 0.214049\n",
      "[1000]\ttraining's rmse: 0.000426358\ttraining's RMSPE: 0.197285\tvalid_1's rmse: 0.000460868\tvalid_1's RMSPE: 0.213463\n",
      "[1200]\ttraining's rmse: 0.000419971\ttraining's RMSPE: 0.19433\tvalid_1's rmse: 0.000459937\tvalid_1's RMSPE: 0.213033\n",
      "[1400]\ttraining's rmse: 0.000414746\ttraining's RMSPE: 0.191912\tvalid_1's rmse: 0.000459116\tvalid_1's RMSPE: 0.212652\n",
      "[1600]\ttraining's rmse: 0.000410197\ttraining's RMSPE: 0.189807\tvalid_1's rmse: 0.000458714\tvalid_1's RMSPE: 0.212466\n",
      "[1800]\ttraining's rmse: 0.000406372\ttraining's RMSPE: 0.188037\tvalid_1's rmse: 0.00045796\tvalid_1's RMSPE: 0.212117\n",
      "[2000]\ttraining's rmse: 0.000402977\ttraining's RMSPE: 0.186466\tvalid_1's rmse: 0.000457601\tvalid_1's RMSPE: 0.21195\n",
      "[2200]\ttraining's rmse: 0.000399794\ttraining's RMSPE: 0.184993\tvalid_1's rmse: 0.000457331\tvalid_1's RMSPE: 0.211825\n",
      "[2400]\ttraining's rmse: 0.000396952\ttraining's RMSPE: 0.183678\tvalid_1's rmse: 0.000457246\tvalid_1's RMSPE: 0.211786\n",
      "[2600]\ttraining's rmse: 0.000394345\ttraining's RMSPE: 0.182472\tvalid_1's rmse: 0.000457194\tvalid_1's RMSPE: 0.211762\n",
      "Early stopping, best iteration is:\n",
      "[2312]\ttraining's rmse: 0.000398222\ttraining's RMSPE: 0.184266\tvalid_1's rmse: 0.000457041\tvalid_1's RMSPE: 0.211691\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[200]\ttraining's rmse: 0.000512895\ttraining's RMSPE: 0.238023\tvalid_1's rmse: 0.000513325\tvalid_1's RMSPE: 0.23496\n",
      "[400]\ttraining's rmse: 0.000460423\ttraining's RMSPE: 0.213672\tvalid_1's rmse: 0.000467402\tvalid_1's RMSPE: 0.21394\n",
      "[600]\ttraining's rmse: 0.000444339\ttraining's RMSPE: 0.206207\tvalid_1's rmse: 0.000461878\tvalid_1's RMSPE: 0.211412\n",
      "[800]\ttraining's rmse: 0.000433729\ttraining's RMSPE: 0.201284\tvalid_1's rmse: 0.000460509\tvalid_1's RMSPE: 0.210785\n",
      "[1000]\ttraining's rmse: 0.000425822\ttraining's RMSPE: 0.197614\tvalid_1's rmse: 0.000459361\tvalid_1's RMSPE: 0.21026\n",
      "[1200]\ttraining's rmse: 0.000419573\ttraining's RMSPE: 0.194714\tvalid_1's rmse: 0.000458519\tvalid_1's RMSPE: 0.209874\n",
      "[1400]\ttraining's rmse: 0.000414305\ttraining's RMSPE: 0.192269\tvalid_1's rmse: 0.000457711\tvalid_1's RMSPE: 0.209504\n",
      "[1600]\ttraining's rmse: 0.000409895\ttraining's RMSPE: 0.190223\tvalid_1's rmse: 0.000456992\tvalid_1's RMSPE: 0.209175\n",
      "[1800]\ttraining's rmse: 0.000406056\ttraining's RMSPE: 0.188441\tvalid_1's rmse: 0.000456504\tvalid_1's RMSPE: 0.208952\n",
      "[2000]\ttraining's rmse: 0.00040263\ttraining's RMSPE: 0.186851\tvalid_1's rmse: 0.000456022\tvalid_1's RMSPE: 0.208731\n",
      "[2200]\ttraining's rmse: 0.000399502\ttraining's RMSPE: 0.1854\tvalid_1's rmse: 0.000455583\tvalid_1's RMSPE: 0.20853\n",
      "[2400]\ttraining's rmse: 0.000396643\ttraining's RMSPE: 0.184073\tvalid_1's rmse: 0.000455356\tvalid_1's RMSPE: 0.208426\n",
      "[2600]\ttraining's rmse: 0.000394087\ttraining's RMSPE: 0.182887\tvalid_1's rmse: 0.000455262\tvalid_1's RMSPE: 0.208383\n",
      "[2800]\ttraining's rmse: 0.000391684\ttraining's RMSPE: 0.181771\tvalid_1's rmse: 0.000455084\tvalid_1's RMSPE: 0.208302\n",
      "[3000]\ttraining's rmse: 0.000389384\ttraining's RMSPE: 0.180704\tvalid_1's rmse: 0.00045499\tvalid_1's RMSPE: 0.208259\n",
      "[3200]\ttraining's rmse: 0.000387335\ttraining's RMSPE: 0.179753\tvalid_1's rmse: 0.000454763\tvalid_1's RMSPE: 0.208155\n",
      "[3400]\ttraining's rmse: 0.000385343\ttraining's RMSPE: 0.178829\tvalid_1's rmse: 0.000454745\tvalid_1's RMSPE: 0.208147\n",
      "[3600]\ttraining's rmse: 0.000383466\ttraining's RMSPE: 0.177958\tvalid_1's rmse: 0.000454706\tvalid_1's RMSPE: 0.208129\n",
      "[3800]\ttraining's rmse: 0.000381701\ttraining's RMSPE: 0.177139\tvalid_1's rmse: 0.000454674\tvalid_1's RMSPE: 0.208114\n",
      "[4000]\ttraining's rmse: 0.000380048\ttraining's RMSPE: 0.176372\tvalid_1's rmse: 0.000454689\tvalid_1's RMSPE: 0.208121\n",
      "[4200]\ttraining's rmse: 0.000378418\ttraining's RMSPE: 0.175615\tvalid_1's rmse: 0.000454657\tvalid_1's RMSPE: 0.208106\n",
      "[4400]\ttraining's rmse: 0.000376887\ttraining's RMSPE: 0.174904\tvalid_1's rmse: 0.000454651\tvalid_1's RMSPE: 0.208104\n",
      "[4600]\ttraining's rmse: 0.000375415\ttraining's RMSPE: 0.174221\tvalid_1's rmse: 0.000454595\tvalid_1's RMSPE: 0.208078\n",
      "[4800]\ttraining's rmse: 0.000373987\ttraining's RMSPE: 0.173559\tvalid_1's rmse: 0.000454611\tvalid_1's RMSPE: 0.208085\n",
      "[5000]\ttraining's rmse: 0.000372609\ttraining's RMSPE: 0.172919\tvalid_1's rmse: 0.000454573\tvalid_1's RMSPE: 0.208068\n",
      "[5200]\ttraining's rmse: 0.000371289\ttraining's RMSPE: 0.172307\tvalid_1's rmse: 0.000454604\tvalid_1's RMSPE: 0.208082\n",
      "Early stopping, best iteration is:\n",
      "[4936]\ttraining's rmse: 0.000373042\ttraining's RMSPE: 0.17312\tvalid_1's rmse: 0.000454495\tvalid_1's RMSPE: 0.208032\n",
      "Our out of folds RMSPE is 0.2140115956153162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2140115956153162"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PARAMS = {\n",
    "        \"objective\": \"rmse\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"max_depth\": -1, # 4 to 20\n",
    "        \"max_bin\":318, # 40 to 400\n",
    "        \"min_data_in_leaf\":2000, # 100 to 2000\n",
    "        \"learning_rate\": 0.01, # 0.01 to 0.1\n",
    "        \"subsample\": 0.66, # 0.3 to 0.9\n",
    "        \"subsample_freq\": 3, # 0 to 30\n",
    "        \"feature_fraction\": 0.567, # 0.5 to 0.9\n",
    "        \"lambda_l1\": 1.27, # 0.0 to 3.0\n",
    "        \"lambda_l2\": 0.475, # 0.0 to 3.0\n",
    "        \"categorical_column\":[0],\n",
    "        \"seed\":2021,\n",
    "        \"feature_fraction_seed\": 2021,\n",
    "        \"bagging_seed\": 2021,\n",
    "        \"drop_seed\": 2021,\n",
    "        \"data_random_seed\": 2021,\n",
    "        \"n_jobs\":-1,\n",
    "        \"verbose\": -1\n",
    "}\n",
    "# micro change from the best hyper-params\n",
    "\n",
    "train_and_evaluate(train, test,\n",
    "                   PARAMS['max_bin'],\n",
    "                   PARAMS['min_data_in_leaf'],\n",
    "                   PARAMS['learning_rate'],\n",
    "                   PARAMS['subsample'],\n",
    "                   PARAMS['subsample_freq'],\n",
    "                   PARAMS['feature_fraction'],\n",
    "                   PARAMS['lambda_l1'],\n",
    "                   PARAMS['lambda_l2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61ad3b17b275c693ade4f72c73fca340e8bff1627c3e46cc2b74f065373987cd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
