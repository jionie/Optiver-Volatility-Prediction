{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7fbf87-8399-4b5a-90cd-8ce5e3ad9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import pandas as pd\n",
    "from pandas.core.common import flatten\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"max_columns\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92f4e0-97b5-4951-bb7c-f3ff176a09c4",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48612360-2cb2-47d3-8e47-836f81121284",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"root_dir\": \"../../input/optiver-realized-volatility-prediction/\",\n",
    "    \"ckpt_path\": \"../../ckpts/\",\n",
    "    \"kfold_seed\": 42,\n",
    "    \"n_splits\": 5,\n",
    "    \"n_clusters\": 7,\n",
    "    \"shuffle\": True,\n",
    "    \"shuffle_seed\": 1997,\n",
    "    \"pretrained\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db39f3d-aaa7-4144-9182-46906cb8880e",
   "metadata": {},
   "source": [
    "# Read Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6aa2bb0-c3f5-46cd-bd8d-ceba27b8903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test():\n",
    "    \n",
    "    train = pd.read_csv(\"../../input/optiver-realized-volatility-prediction/train.csv\")\n",
    "    test = pd.read_csv(\"../../input/optiver-realized-volatility-prediction/test.csv\")\n",
    "    \n",
    "    # Create a key to merge with book and trade data\n",
    "    train[\"row_id\"] = train[\"stock_id\"].astype(str) + \"-\" + train[\"time_id\"].astype(str)\n",
    "    test[\"row_id\"] = test[\"stock_id\"].astype(str) + \"-\" + test[\"time_id\"].astype(str)\n",
    "    \n",
    "    print(\"Our training set has {} rows\".format(train.shape[0]))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0382e-681a-478d-b7d5-ed6e6963c708",
   "metadata": {},
   "source": [
    "# Define basic metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3eea82-a842-4980-ba01-05850f8f29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_counts(df):\n",
    "    activity_counts_ = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].agg(\"count\").reset_index()\n",
    "    activity_counts_ = activity_counts_.rename(columns={\"seconds_in_bucket\": \"activity_counts\"})\n",
    "    return activity_counts_\n",
    "\n",
    "\n",
    "def calc_wap(df, pos=1):\n",
    "    wap = (df[\"bid_price{}\".format(pos)] * df[\"ask_size{}\".format(pos)] + df[\"ask_price{}\".format(pos)] * df[\n",
    "        \"bid_size{}\".format(pos)]) / (df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df, pos=1):\n",
    "    wap = (df[\"bid_price{}\".format(pos)] * df[\"bid_size{}\".format(pos)] + df[\"ask_price{}\".format(pos)] * df[\n",
    "        \"ask_size{}\".format(pos)]) / (df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def wp(df):\n",
    "    wp_ = (df[\"bid_price1\"] * df[\"bid_size1\"] + df[\"ask_price1\"] * df[\"ask_size1\"] + df[\"bid_price2\"] * df[\n",
    "        \"bid_size2\"] + df[\"ask_price2\"] * df[\"ask_size2\"]) / (\n",
    "                  df[\"bid_size1\"] + df[\"ask_size1\"] + df[\"bid_size2\"] + df[\"ask_size2\"])\n",
    "    return wp_\n",
    "\n",
    "\n",
    "def maximum_drawdown(series, window=600):\n",
    "    # window for 10 minutes, use min_periods=1 if you want to allow the expanding window\n",
    "    roll_max = series.rolling(window, min_periods=1).max()\n",
    "    second_drawdown = series / roll_max - 1.0\n",
    "    max_drawdown = second_drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff().fillna(0)\n",
    "\n",
    "\n",
    "def rolling_log_return(series, rolling=60):\n",
    "    return np.log(series.rolling(rolling)).diff().fillna(0)\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series ** 2))\n",
    "\n",
    "\n",
    "def diff(series):\n",
    "    return series.diff().fillna(0)\n",
    "\n",
    "\n",
    "def time_diff(series):\n",
    "    return series.diff().fillna(series)\n",
    "\n",
    "\n",
    "def order_flow_imbalance(df, pos=1):\n",
    "    df[\"bid_price{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"bid_price{}\".format(pos)].apply(diff)\n",
    "    df[\"bid_size{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"bid_price{}\".format(pos)].apply(diff)\n",
    "    df[\"bid_order_flow{}\".format(pos)] = df[\"bid_size{}\".format(pos)].copy(deep=True)\n",
    "    df[\"bid_order_flow{}\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] < 0] *= -1\n",
    "    df[\"bid_order_flow{}\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] == 0] = \\\n",
    "        df[\"bid_size{}_diff\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] == 0]\n",
    "\n",
    "    df[\"ask_price{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"ask_price{}\".format(pos)].apply(diff)\n",
    "    df[\"ask_size{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"ask_price{}\".format(pos)].apply(diff)\n",
    "    df[\"ask_order_flow{}\".format(pos)] = df[\"ask_size{}\".format(pos)].copy(deep=True)\n",
    "    df[\"ask_order_flow{}\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] < 0] *= -1\n",
    "    df[\"ask_order_flow{}\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] == 0] = \\\n",
    "        df[\"ask_size{}_diff\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] == 0]\n",
    "\n",
    "    order_flow_imbalance_ = df[\"bid_order_flow{}\".format(pos)] - df[\"ask_order_flow{}\".format(pos)]\n",
    "\n",
    "    df.drop([\"bid_price{}_diff\".format(pos), \"bid_size{}_diff\".format(pos), \"bid_order_flow{}\".format(pos),\n",
    "             \"ask_price{}_diff\".format(pos), \"ask_size{}_diff\".format(pos), \"ask_order_flow{}\".format(pos)], axis=1,\n",
    "            inplace=True)\n",
    "\n",
    "    return order_flow_imbalance_ + 1e-8\n",
    "\n",
    "\n",
    "def order_book_slope(df):\n",
    "\n",
    "    df[\"mid_point\"] = (df[\"bid_price1\"] + df[\"ask_price1\"]) / 2\n",
    "    best_mid_point_ = df.groupby([\"time_id\"])[\"mid_point\"].agg(\"max\").reset_index()\n",
    "    best_mid_point_ = best_mid_point_.rename(columns={\"mid_point\": \"best_mid_point\"})\n",
    "    df = df.merge(best_mid_point_, how=\"left\", on=\"time_id\")\n",
    "\n",
    "    best_mid_point = df[\"best_mid_point\"].copy()\n",
    "    df.drop([\"mid_point\", \"best_mid_point\"], axis=1, inplace=True)\n",
    "\n",
    "    def ratio(series):\n",
    "        ratio_ = series / series.shift()\n",
    "        return ratio_\n",
    "\n",
    "    bid_price1_ratio = df.groupby([\"time_id\"])[\"bid_price1\"].apply(ratio)\n",
    "    bid_price1_mid_point_ratio = df[\"bid_price1\"] / best_mid_point\n",
    "    bid_price1_ratio = abs(bid_price1_ratio.fillna(bid_price1_mid_point_ratio) - 1)\n",
    "\n",
    "    bid_size1_ratio = df.groupby([\"time_id\"])[\"bid_size1\"].apply(ratio) - 1\n",
    "    bid_size1_ratio = bid_size1_ratio.fillna(df[\"bid_size1\"])\n",
    "    df[\"DE\"] = (bid_size1_ratio / bid_price1_ratio).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    ask_price1_ratio = df.groupby([\"time_id\"])[\"ask_price1\"].apply(ratio)\n",
    "    ask_price1_mid_point_ratio = df[\"ask_price1\"] / best_mid_point\n",
    "    ask_price1_ratio = abs(ask_price1_ratio.fillna(ask_price1_mid_point_ratio) - 1)\n",
    "\n",
    "    ask_size1_ratio = df.groupby([\"time_id\"])[\"ask_size1\"].apply(ratio) - 1\n",
    "    ask_size1_ratio = ask_size1_ratio.fillna(df[\"ask_size1\"])\n",
    "    df[\"SE\"] = (ask_size1_ratio / ask_price1_ratio).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    df[\"order_book_slope\"] = (df[\"DE\"] + df[\"SE\"]) / 2\n",
    "    order_book_slope_ = df.groupby([\"time_id\"])[\"order_book_slope\"].agg(\"mean\").reset_index()\n",
    "    df.drop([\"order_book_slope\", \"DE\", \"SE\"], axis=1, inplace=True)\n",
    "\n",
    "    return order_book_slope_\n",
    "\n",
    "\n",
    "def ldispersion(df):\n",
    "    LDispersion = 1 / 2 * (\n",
    "            df[\"bid_size1\"] / (df[\"bid_size1\"] + df[\"bid_size2\"]) * abs(df[\"bid_price1\"] - df[\"bid_price2\"]) + df[\n",
    "        \"ask_size1\"] / (df[\"ask_size1\"] + df[\"ask_size2\"]) * abs(df[\"ask_price1\"] - df[\"ask_price2\"]))\n",
    "    return LDispersion\n",
    "\n",
    "\n",
    "def depth_imbalance(df, pos=1):\n",
    "    depth_imbalance_ = (df[\"bid_size{}\".format(pos)] - df[\"ask_size{}\".format(pos)]) / (\n",
    "            df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "\n",
    "    return depth_imbalance_\n",
    "\n",
    "\n",
    "def height_imbalance(df, pos=1):\n",
    "    height_imbalance_ = (df[\"bid_price{}\".format(pos)] - df[\"ask_price{}\".format(pos)]) / (\n",
    "            df[\"bid_price{}\".format(pos)] + df[\"ask_price{}\".format(pos)])\n",
    "\n",
    "    return height_imbalance_\n",
    "\n",
    "\n",
    "def pressure_imbalance(df):\n",
    "    mid_price = (df[\"bid_price1\"] + df[\"ask_price1\"]) / 2\n",
    "\n",
    "    weight_buy = mid_price / (mid_price - df[\"bid_price1\"]) + mid_price / (mid_price - df[\"bid_price2\"])\n",
    "    pressure_buy = df[\"bid_size1\"] * (mid_price / (mid_price - df[\"bid_price1\"])) / weight_buy + df[\"bid_size2\"] * (\n",
    "            mid_price / (mid_price - df[\"bid_price2\"])) / weight_buy\n",
    "\n",
    "    weight_sell = mid_price / (df[\"ask_price1\"] - mid_price) + mid_price / (df[\"ask_price2\"] - mid_price)\n",
    "    pressure_sell = df[\"ask_size1\"] * (mid_price / (df[\"ask_price1\"] - mid_price)) / weight_sell + df[\"ask_size2\"] * (\n",
    "            mid_price / (df[\"ask_price2\"] - mid_price)) / weight_sell\n",
    "\n",
    "    pressure_imbalance_ = np.log(pressure_buy) - np.log(pressure_sell)\n",
    "\n",
    "    return pressure_imbalance_\n",
    "\n",
    "\n",
    "def relative_spread(df, pos=1):\n",
    "    relative_spread_ = 2 * (df[\"ask_price{}\".format(pos)] - df[\"bid_price{}\".format(pos)]) / (\n",
    "            df[\"ask_price{}\".format(pos)] + df[\"bid_price{}\".format(pos)])\n",
    "\n",
    "    return relative_spread_\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc4c77-114b-44e6-a238-6ac10de336ce",
   "metadata": {},
   "source": [
    "# Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4628652-1114-4c18-a488-71833798085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # float 64 to float 32\n",
    "    float_cols = df.select_dtypes(include=[np.float64]).columns\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "    # int 64 to int 32\n",
    "    int_cols = df.select_dtypes(include=[np.int64]).columns\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    # rebase seconds_in_bucket\n",
    "    df[\"seconds_in_bucket\"] = df[\"seconds_in_bucket\"] - df[\"seconds_in_bucket\"].min()\n",
    "\n",
    "    # Calculate seconds gap\n",
    "    df[\"seconds_gap\"] = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].apply(time_diff)\n",
    "\n",
    "    # Calculate Wap\n",
    "    df[\"wap1\"] = calc_wap(df, pos=1)\n",
    "    df[\"wap2\"] = calc_wap(df, pos=2)\n",
    "\n",
    "    # Calculate wap balance\n",
    "    df[\"wap_balance\"] = abs(df[\"wap1\"] - df[\"wap2\"])\n",
    "\n",
    "    # Calculate log returns\n",
    "    df[\"log_return1\"] = df.groupby([\"time_id\"])[\"wap1\"].apply(log_return)\n",
    "    df[\"log_return2\"] = df.groupby([\"time_id\"])[\"wap2\"].apply(log_return)\n",
    "\n",
    "    # Calculate spread\n",
    "    df[\"bid_ask_spread1\"] = df[\"ask_price1\"] / df[\"bid_price1\"] - 1\n",
    "    df[\"bid_ask_spread2\"] = df[\"ask_price2\"] / df[\"bid_price2\"] - 1\n",
    "\n",
    "    # order flow imbalance\n",
    "    df[\"order_flow_imbalance1\"] = order_flow_imbalance(df, 1)\n",
    "    df[\"order_flow_imbalance2\"] = order_flow_imbalance(df, 2)\n",
    "\n",
    "    # order book slope\n",
    "    order_slope_ = order_book_slope(df)\n",
    "    df = df.merge(order_slope_, how=\"left\", on=\"time_id\")\n",
    "\n",
    "    # depth imbalance\n",
    "    df[\"depth_imbalance1\"] = depth_imbalance(df, pos=1)\n",
    "    df[\"depth_imbalance2\"] = depth_imbalance(df, pos=2)\n",
    "\n",
    "    # height imbalance\n",
    "    df[\"height_imbalance1\"] = height_imbalance(df, pos=1)\n",
    "    df[\"height_imbalance2\"] = height_imbalance(df, pos=2)\n",
    "\n",
    "    # pressure imbalance\n",
    "    df[\"pressure_imbalance\"] = pressure_imbalance(df)\n",
    "\n",
    "    # total volume\n",
    "    df[\"total_volume\"] = (df[\"ask_size1\"] + df[\"ask_size2\"]) + (df[\"bid_size1\"] + df[\"bid_size2\"])\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        \"wap1\": [np.sum, np.std],\n",
    "        \"wap2\": [np.sum, np.std],\n",
    "        \"log_return1\": [realized_volatility],\n",
    "        \"log_return2\": [realized_volatility],\n",
    "        \"wap_balance\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_book_slope\": [np.mean, np.max],\n",
    "        \"depth_imbalance1\": [np.sum, np.max, np.std],\n",
    "        \"depth_imbalance2\": [np.sum, np.max, np.std],\n",
    "        \"height_imbalance1\": [np.sum, np.max, np.std],\n",
    "        \"height_imbalance2\": [np.sum, np.max, np.std],\n",
    "        \"pressure_imbalance\": [np.sum, np.max, np.std],\n",
    "        \"total_volume\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        \"log_return1\": [realized_volatility],\n",
    "        \"log_return2\": [realized_volatility],\n",
    "        \"wap_balance\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"total_volume\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(feature_dict, seconds_in_bucket, add_suffix=False):\n",
    "        # Group by the window\n",
    "        df_feature_ = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(\n",
    "            feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature_.columns = [\"_\".join(col) for col in df_feature_.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature_ = df_feature_.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "        return df_feature_\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    windows = [0, 150, 300, 450]\n",
    "    add_suffixes = [False, True, True, True]\n",
    "    df_feature = None\n",
    "\n",
    "    for window, add_suffix in zip(windows, add_suffixes):\n",
    "        if df_feature is None:\n",
    "            df_feature = get_stats_window(feature_dict=create_feature_dict, seconds_in_bucket=window,\n",
    "                                          add_suffix=add_suffix)\n",
    "        else:\n",
    "            new_df_feature = get_stats_window(feature_dict=create_feature_dict_time, seconds_in_bucket=window,\n",
    "                                              add_suffix=add_suffix)\n",
    "            df_feature = df_feature.merge(new_df_feature, how=\"left\", left_on=\"time_id_\",\n",
    "                                          right_on=\"time_id__{}\".format(window))\n",
    "\n",
    "            # Drop unnecesary time_ids\n",
    "            df_feature.drop([\"time_id__{}\".format(window)], axis=1, inplace=True)\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature.drop([\"time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97fcef12-2b0b-477f-9e24-ad8eadc4c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # float 64 to float 32\n",
    "    float_cols = df.select_dtypes(include=[np.float64]).columns\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "    # int 64 to int 32\n",
    "    int_cols = df.select_dtypes(include=[np.int64]).columns\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    # rebase seconds_in_bucket\n",
    "    df[\"seconds_in_bucket\"] = df[\"seconds_in_bucket\"] - df[\"seconds_in_bucket\"].min()\n",
    "\n",
    "    # Calculate seconds gap\n",
    "    df[\"seconds_gap\"] = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].apply(time_diff)\n",
    "\n",
    "    # Calculate log return\n",
    "    df[\"price_log_return\"] = df.groupby(\"time_id\")[\"price\"].apply(log_return)\n",
    "\n",
    "    # Calculate volumes\n",
    "    df[\"volumes\"] = df[\"price\"] * df[\"size\"]\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        \"price_log_return\": [realized_volatility],\n",
    "        \"volumes\": [np.sum, np.max, np.std],\n",
    "        \"order_count\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        \"price_log_return\": [realized_volatility],\n",
    "        \"volumes\": [np.sum, np.max, np.std],\n",
    "        \"order_count\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(feature_dict, seconds_in_bucket, add_suffix=False):\n",
    "        # Group by the window\n",
    "        df_feature_ = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(\n",
    "            feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature_.columns = [\"_\".join(col) for col in df_feature_.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature_ = df_feature_.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "        return df_feature_\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    windows = [0, 150, 300, 450]\n",
    "    add_suffixes = [False, True, True, True]\n",
    "    df_feature = None\n",
    "\n",
    "    for window, add_suffix in zip(windows, add_suffixes):\n",
    "        if df_feature is None:\n",
    "            df_feature = get_stats_window(feature_dict=create_feature_dict, seconds_in_bucket=window,\n",
    "                                          add_suffix=add_suffix)\n",
    "        else:\n",
    "            new_df_feature = get_stats_window(feature_dict=create_feature_dict_time, seconds_in_bucket=window,\n",
    "                                              add_suffix=add_suffix)\n",
    "            df_feature = df_feature.merge(new_df_feature, how=\"left\", left_on=\"time_id_\",\n",
    "                                          right_on=\"time_id__{}\".format(window))\n",
    "\n",
    "            # Drop unnecesary time_ids\n",
    "            df_feature.drop([\"time_id__{}\".format(window)], axis=1, inplace=True)\n",
    "\n",
    "    def tendency(price, vol):\n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff / price[1:]) * 100\n",
    "        power = np.sum(val * vol[1:])\n",
    "        return (power)\n",
    "\n",
    "    lis = []\n",
    "    for n_time_id in df[\"time_id\"].unique():\n",
    "        \n",
    "        df_id = df[df[\"time_id\"] == n_time_id]\n",
    "        \n",
    "        tendencyV = tendency(df_id[\"price\"].values, df_id[\"size\"].values)\n",
    "        energy = np.mean(df_id[\"price\"].values ** 2)\n",
    "\n",
    "        lis.append(\n",
    "            {\n",
    "                \"time_id\": n_time_id,\n",
    "                \"tendency\": tendencyV,\n",
    "                \"energy\": energy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_feature = df_feature.merge(df_lr, how=\"left\", left_on=\"time_id_\", right_on=\"time_id\")\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    df_feature = df_feature.add_prefix(\"trade_\")\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"trade_time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature.drop([\"trade_time_id_\", \"trade_time_id\"], axis=1, inplace=True)\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2dadda6-499a-4969-bef7-2d47d8178ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = CONFIG[\"root_dir\"] + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = CONFIG[\"root_dir\"] + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = CONFIG[\"root_dir\"] + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = CONFIG[\"root_dir\"] + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = \"row_id\", how = \"left\")\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    \n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2a4b5d-63aa-4992-97b6-a3df4e43f1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   52.0s\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  4.8min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train[\"stock_id\"].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train=True)\n",
    "train = train.merge(train_, on=[\"row_id\"], how=\"left\")\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test[\"stock_id\"].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train=False)\n",
    "test = test.merge(test_, on=[\"row_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278df512-5e94-40a6-b9da-2ad0350c1346",
   "metadata": {},
   "source": [
    "# Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e56f947-264b-4a2f-83f9-4d4ec06ad913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs log columns\n",
    "abs_log_columns = [column for column in train.columns if \n",
    "                       \"order_flow_imbalance\" in column or \n",
    "                       \"order_book_slope\" in column or \n",
    "                       \"depth_imbalance\" in column or \n",
    "                       \"pressure_imbalance\" in column or\n",
    "                       \"total_volume\" in column or\n",
    "                       \"seconds_gap\" in column or\n",
    "                       \"trade_volumes\" in column or\n",
    "                       \"trade_order_count\" in column or\n",
    "                       \"trade_seconds_gap\" in column or\n",
    "                       \"trade_tendency\" in column\n",
    "                      ]\n",
    "\n",
    "# apply abs + log1p\n",
    "train[abs_log_columns] = (train[abs_log_columns].apply(np.abs)).apply(np.log1p)\n",
    "test[abs_log_columns] = (test[abs_log_columns].apply(np.abs)).apply(np.log1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829b635-1244-4389-8f98-f7a5f9114ab3",
   "metadata": {},
   "source": [
    "# Fill inf with nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a02e48-a788-4572-a1ce-d5b554944430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8263237-9f5a-4790-b491-1cfb9636a379",
   "metadata": {},
   "source": [
    "# Agg features inside fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64e7584-8a75-4192-b88f-19ccb01653fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process agg by kmeans\n",
    "def get_kmeans_idx(n_clusters=7):\n",
    "    train_p = pd.read_csv(\"../../input/optiver-realized-volatility-prediction/train.csv\")\n",
    "    train_p = train_p.pivot(index=\"time_id\", columns=\"stock_id\", values=\"target\")\n",
    "\n",
    "    corr = train_p.corr()\n",
    "\n",
    "    ids = corr.index\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(corr.values)\n",
    "\n",
    "    kmeans_clusters = []\n",
    "    for n in range(n_clusters):\n",
    "        kmeans_clusters.append ([(x - 1) for x in ((ids + 1)*(kmeans.labels_ == n)) if x > 0])\n",
    "        \n",
    "    return kmeans_clusters\n",
    "    \n",
    "\n",
    "def agg_stat_features_by_clusters(df, n_clusters=7, function=np.nanmean, post_fix=\"_cluster_mean\"):\n",
    "\n",
    "    kmeans_clusters = get_kmeans_idx(n_clusters=n_clusters)\n",
    "\n",
    "    clusters = []\n",
    "    agg_columns = [\n",
    "        \"time_id\",\n",
    "        \"stock_id\",\n",
    "        \"log_return1_realized_volatility\",\n",
    "        \"log_return2_realized_volatility\",\n",
    "        \"order_flow_imbalance1_sum\",\n",
    "        \"order_flow_imbalance2_sum\",\n",
    "        \"order_book_slope_mean\",\n",
    "        \"depth_imbalance1_std\",\n",
    "        \"depth_imbalance2_std\",\n",
    "        \"height_imbalance1_sum\",\n",
    "        \"height_imbalance2_sum\",\n",
    "        \"pressure_imbalance_std\",\n",
    "        \"total_volume_sum\",\n",
    "        \"seconds_gap_mean\",\n",
    "        \"trade_price_log_return_realized_volatility\",\n",
    "        \"trade_volumes_sum\",\n",
    "        \"trade_order_count_sum\",\n",
    "        \"trade_seconds_gap_mean\",\n",
    "        \"trade_tendency\",\n",
    "        \"trade_energy\"\n",
    "    ]\n",
    "\n",
    "    for cluster_idx, ind in enumerate(kmeans_clusters):\n",
    "        cluster_df = df.loc[df[\"stock_id\"].isin(ind), agg_columns].groupby([\"time_id\"]).agg(function)\n",
    "        cluster_df.loc[:, \"stock_id\"] = str(cluster_idx) + post_fix\n",
    "        clusters.append(cluster_df)\n",
    "\n",
    "    clusters_df = pd.concat(clusters).reset_index()\n",
    "    # multi index (column, c1)\n",
    "    clusters_df = clusters_df.pivot(index=\"time_id\", columns=\"stock_id\")\n",
    "    # ravel multi index to list of tuple [(target, c1), ...]\n",
    "    clusters_df.columns = [\"_\".join(x) for x in clusters_df.columns.ravel()]\n",
    "    clusters_df.reset_index(inplace=True)\n",
    "\n",
    "    postfixes = [\n",
    "        \"0\" + post_fix,\n",
    "        \"1\" + post_fix,\n",
    "        \"3\" + post_fix,\n",
    "        \"4\" + post_fix,\n",
    "        \"6\" + post_fix,\n",
    "    ]\n",
    "    merge_columns = []\n",
    "    for column in agg_columns:\n",
    "        if column == \"time_id\":\n",
    "            merge_columns.append(column)\n",
    "        elif column == \"stock_id\":\n",
    "            continue\n",
    "        else:\n",
    "            for postfix in postfixes:\n",
    "                merge_columns.append(column + \"_\" + postfix)\n",
    "                \n",
    "    not_exist_columns = [column for column in merge_columns if column not in clusters_df.columns]\n",
    "    clusters_df[not_exist_columns] = 0\n",
    "    \n",
    "    df = pd.merge(df, clusters_df[merge_columns], how=\"left\", on=\"time_id\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to get group stats for the time_id\n",
    "def agg_stat_features_by_market(df, operations=None, operations_names=None):\n",
    "    def percentile(n):\n",
    "        def percentile_(x):\n",
    "            return np.percentile(x, n)\n",
    "\n",
    "        percentile_.__name__ = \"percentile_%s\" % n\n",
    "        return percentile_\n",
    "\n",
    "    if operations is None:\n",
    "        operations = [\n",
    "            np.nanmean,\n",
    "        ]\n",
    "        operations_names = [\n",
    "            \"mean\",\n",
    "        ]\n",
    "\n",
    "    # Get realized volatility columns\n",
    "    vol_cols = [\n",
    "        \"log_return1_realized_volatility\",\n",
    "        \"log_return1_realized_volatility_150\",\n",
    "        \"log_return1_realized_volatility_300\",\n",
    "        \"log_return1_realized_volatility_450\",\n",
    "    ]\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby([\"stock_id\"])[vol_cols].agg(operations).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = [\"_\".join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix(\"_\" + \"stock\")\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby([\"time_id\"])[vol_cols].agg(operations).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = [\"_\".join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix(\"_\" + \"time\")\n",
    "\n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how=\"left\", left_on=[\"stock_id\"], right_on=[\"stock_id__stock\"])\n",
    "    df.drop(\"stock_id__stock\", axis=1, inplace=True)\n",
    "\n",
    "    df = df.merge(df_time_id, how=\"left\", left_on=[\"time_id\"], right_on=[\"time_id__time\"])\n",
    "    df.drop(\"time_id__time\", axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a95433-524b-4d66-a6d2-3208e3449ed2",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "398a9da6-4930-4da3-8bc6-5a9b210b062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        \n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "    \n",
    "\n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8775544d-92b2-44b3-9f66-8b2ec8422748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test):\n",
    "    \n",
    "    # label encoder\n",
    "    cat_columns = [\"stock_id\"]\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder = label_encoder.fit(train[cat_columns].values)\n",
    "    train[cat_columns] = label_encoder.transform(train[cat_columns].values)\n",
    "    dump(label_encoder, os.path.join(CONFIG[\"ckpt_path\"], \"label_encoder.bin\"), compress=True)\n",
    "    test[cat_columns] = label_encoder.transform(test[cat_columns].values)\n",
    "    cat_dims = [len(label_encoder.classes_)]\n",
    "    \n",
    "    # scaler\n",
    "    # scaler = QuantileTransformer(n_quantiles=2000, random_state=2021)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Split features and target\n",
    "    x = train.drop([\"row_id\", \"target\"], axis=1)\n",
    "    y = train[\"target\"]\n",
    "    \n",
    "    # x_test with train feature\n",
    "    x_test = test.drop(\"row_id\", axis=1)\n",
    "    x_test = agg_stat_features_by_market(x_test)\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "\n",
    "    # define normalize columns\n",
    "    except_columns = [\"stock_id\", \"time_id\", \"target\", \"row_id\"]\n",
    "    normalized_columns = [column for column in x_test.columns if column not in except_columns]\n",
    "    x_test.drop(\"time_id\", axis=1, inplace=True)\n",
    "    \n",
    "    # Process categorical features and get params dict\n",
    "    cat_idxs = [i for i, f in enumerate(x_test.columns.tolist()) if f in cat_columns]\n",
    "    \n",
    "    params = dict(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        cat_emb_dim=1,\n",
    "        n_d=16,\n",
    "        n_a=16,\n",
    "        n_steps=2,\n",
    "        gamma=2,\n",
    "        n_independent=2,\n",
    "        n_shared=2,\n",
    "        lambda_sparse=0,\n",
    "        optimizer_fn=Adam,\n",
    "        optimizer_params=dict(lr = (2e-2)),\n",
    "        mask_type=\"entmax\",\n",
    "        scheduler_params=dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "        scheduler_fn=CosineAnnealingWarmRestarts,\n",
    "        seed=42,\n",
    "        verbose=10\n",
    "    )\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    \n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    \n",
    "    # Statistics\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances[\"feature\"] = x_test.columns.tolist()\n",
    "    stats = pd.DataFrame()\n",
    "    explain_matrices = []\n",
    "    masks_ =[]\n",
    "    \n",
    "    # Create a KFold object\n",
    "    kfold = GroupKFold(n_splits=CONFIG[\"n_splits\"])\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x, groups=x[\"time_id\"])):\n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        x_train = x.iloc[trn_ind]\n",
    "        x_train = agg_stat_features_by_market(x_train)\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "        x_train.drop(\"time_id\", axis=1, inplace=True)\n",
    "        \n",
    "        scaler = scaler.fit(x_train[normalized_columns])\n",
    "        dump(scaler, os.path.join(CONFIG[\"ckpt_path\"], \"tabnet_std_scaler_fold_{}.bin\".format(fold + 1)), compress=True)\n",
    "        x_train[normalized_columns] = scaler.transform(x_train[normalized_columns])\n",
    "        x_train = x_train.fillna(0)\n",
    "        \n",
    "        x_val = x.iloc[val_ind]\n",
    "        x_val = agg_stat_features_by_market(x_val)\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "        x_val.drop(\"time_id\", axis=1, inplace=True)\n",
    "        \n",
    "        x_val[normalized_columns] = scaler.transform(x_val[normalized_columns])\n",
    "        x_val = x_val.fillna(0)\n",
    "        \n",
    "        y_train, y_val = y.iloc[trn_ind].values.reshape(-1, 1), y.iloc[val_ind].values.reshape(-1, 1)\n",
    "        \n",
    "        # Train\n",
    "        clf =  TabNetRegressor(**params)\n",
    "        \n",
    "        if CONFIG[\"pretrained\"] and os.path.exists(os.path.join(CONFIG[\"ckpt_path\"], \"tabnet_fold{}.zip\".format(fold + 1))):\n",
    "            clf.load_model(os.path.join(CONFIG[\"ckpt_path\"], \"tabnet_fold{}.zip\".format(fold + 1)))\n",
    "        else:\n",
    "            clf.fit(\n",
    "                  x_train.values, y_train,\n",
    "                  eval_set=[(x_val.values, y_val)],\n",
    "                  max_epochs=200,\n",
    "                  patience=50,\n",
    "                  batch_size=1024*20, \n",
    "                  virtual_batch_size=128*20,\n",
    "                  num_workers=0,\n",
    "                  drop_last=False,\n",
    "                  eval_metric=[RMSPE],\n",
    "                  loss_fn=RMSPELoss\n",
    "              )\n",
    "\n",
    "            # save model\n",
    "            saved_filepath = clf.save_model(os.path.join(CONFIG[\"ckpt_path\"], \"tabnet_fold{}\".format(fold + 1)))\n",
    "        \n",
    "        # save statistics\n",
    "        explain_matrix, masks = clf.explain(x_val.values)\n",
    "        explain_matrices.append(explain_matrix)\n",
    "        masks_.append(masks[0])\n",
    "        masks_.append(masks[1])\n",
    "        \n",
    "        # save oof and test predictions\n",
    "        oof_predictions[val_ind] = clf.predict(x_val.values).flatten()\n",
    "        x_test_ = x_test.copy()\n",
    "        \n",
    "        x_test_[normalized_columns] = scaler.transform(x_test_[normalized_columns])\n",
    "        x_test_ = x_test_.fillna(0)\n",
    "        \n",
    "        test_predictions += clf.predict(x_test_.values).flatten() / CONFIG[\"n_splits\"]\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(\"Our out of folds RMSPE is {}\".format(rmspe_score))\n",
    "    \n",
    "    # Return test predictions\n",
    "    return test_predictions, stats, feature_importances, explain_matrices, masks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15fb8ffa-a102-402c-82b6-aedf895d02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling dataset\n",
      "Training fold 1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 145.1339| val_0_rmspe: 45.0331 |  0:00:09s\n",
      "epoch 10 | loss: 1.27266 | val_0_rmspe: 1.16858 |  0:01:33s\n",
      "epoch 20 | loss: 1.10778 | val_0_rmspe: 1.75913 |  0:02:57s\n",
      "epoch 30 | loss: 0.45416 | val_0_rmspe: 0.44331 |  0:04:19s\n",
      "epoch 40 | loss: 0.39822 | val_0_rmspe: 0.3414  |  0:05:42s\n",
      "epoch 50 | loss: 0.32769 | val_0_rmspe: 0.30663 |  0:07:04s\n",
      "epoch 60 | loss: 0.26401 | val_0_rmspe: 0.26592 |  0:08:27s\n",
      "epoch 70 | loss: 0.24352 | val_0_rmspe: 0.24514 |  0:09:49s\n",
      "epoch 80 | loss: 0.22456 | val_0_rmspe: 0.23949 |  0:11:12s\n",
      "epoch 90 | loss: 0.21725 | val_0_rmspe: 0.22892 |  0:12:34s\n",
      "epoch 100| loss: 0.21938 | val_0_rmspe: 0.23077 |  0:13:57s\n",
      "epoch 110| loss: 0.20914 | val_0_rmspe: 0.22704 |  0:15:20s\n",
      "epoch 120| loss: 0.20693 | val_0_rmspe: 0.23098 |  0:16:42s\n",
      "epoch 130| loss: 0.20607 | val_0_rmspe: 0.22915 |  0:18:05s\n",
      "epoch 140| loss: 0.20276 | val_0_rmspe: 0.22801 |  0:19:27s\n",
      "epoch 150| loss: 0.19982 | val_0_rmspe: 0.22939 |  0:20:50s\n",
      "epoch 160| loss: 0.1983  | val_0_rmspe: 0.22978 |  0:22:12s\n",
      "\n",
      "Early stopping occurred at epoch 167 with best_epoch = 117 and best_val_0_rmspe = 0.22656\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ../../ckpts/tabnet_fold1.zip\n",
      "Training fold 2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 138.36547| val_0_rmspe: 31.84468|  0:00:09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-14-2398305526da>\", line 5, in <module>\n",
      "    test_predictions, stats, feature_importances, explain_matrices, masks_ = train_and_evaluate(train, test)\n",
      "  File \"<ipython-input-13-faaa7f63fd3e>\", line 107, in train_and_evaluate\n",
      "    clf.fit(\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\", line 223, in fit\n",
      "    self._train_epoch(train_dataloader)\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\", line 431, in _train_epoch\n",
      "    for batch_idx, (X, y) in enumerate(train_loader):\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 521, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 561, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 47, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 84, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 84, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 64, in default_collate\n",
      "    return default_collate([torch.as_tensor(b) for b in batch])\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 56, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\jioni\\anaconda3\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-2398305526da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shuffle_seed\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtest_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplain_matrices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-faaa7f63fd3e>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             clf.fit(\n\u001b[0m\u001b[0;32m    108\u001b[0m                   \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callback_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2060\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2063\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "if CONFIG[\"shuffle\"]:\n",
    "    print(\"shuffling dataset\")\n",
    "    train = train.sample(frac=1, random_state=CONFIG[\"shuffle_seed\"]).reset_index(drop=True)\n",
    "test_predictions, stats, feature_importances, explain_matrices, masks_ = train_and_evaluate(train, test)\n",
    "\n",
    "# Save test predictions\n",
    "# test[\"target\"] = test_predictions\n",
    "# test[[\"row_id\", \"target\"]].to_csv(\"submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172554d-d015-404d-b3be-edde8ab5bd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
