{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7fbf87-8399-4b5a-90cd-8ce5e3ad9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import pandas as pd\n",
    "from pandas.core.common import flatten\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"max_columns\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92f4e0-97b5-4951-bb7c-f3ff176a09c4",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48612360-2cb2-47d3-8e47-836f81121284",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"root_dir\": \"../../input/optiver-realized-volatility-prediction/\",\n",
    "    \"ckpt_path\": \"../../ckpts/\",\n",
    "    \"kfold_seed\": 42,\n",
    "    \"n_splits\": 5,\n",
    "    \"n_clusters\": 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db39f3d-aaa7-4144-9182-46906cb8880e",
   "metadata": {},
   "source": [
    "# Read Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6aa2bb0-c3f5-46cd-bd8d-ceba27b8903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test():\n",
    "    \n",
    "    train = pd.read_csv(\"../../input/optiver-realized-volatility-prediction/train.csv\")\n",
    "    test = pd.read_csv(\"../../input/optiver-realized-volatility-prediction/test.csv\")\n",
    "    \n",
    "    # Create a key to merge with book and trade data\n",
    "    train[\"row_id\"] = train[\"stock_id\"].astype(str) + \"-\" + train[\"time_id\"].astype(str)\n",
    "    test[\"row_id\"] = test[\"stock_id\"].astype(str) + \"-\" + test[\"time_id\"].astype(str)\n",
    "    \n",
    "    print(\"Our training set has {} rows\".format(train.shape[0]))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0382e-681a-478d-b7d5-ed6e6963c708",
   "metadata": {},
   "source": [
    "# Define basic metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3eea82-a842-4980-ba01-05850f8f29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_counts(df):\n",
    "    activity_counts_ = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].agg(\"count\").reset_index()\n",
    "    activity_counts_ = activity_counts_.rename(columns={\"seconds_in_bucket\": \"activity_counts\"})\n",
    "    return activity_counts_\n",
    "\n",
    "\n",
    "def calc_wap(df, pos=1):\n",
    "    wap = (df[\"bid_price{}\".format(pos)] * df[\"ask_size{}\".format(pos)] + df[\"ask_price{}\".format(pos)] * df[\n",
    "        \"bid_size{}\".format(pos)]) / (df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df, pos=1):\n",
    "    wap = (df[\"bid_price{}\".format(pos)] * df[\"bid_size{}\".format(pos)] + df[\"ask_price{}\".format(pos)] * df[\n",
    "        \"ask_size{}\".format(pos)]) / (df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def wp(df):\n",
    "    wp_ = (df[\"bid_price1\"] * df[\"bid_size1\"] + df[\"ask_price1\"] * df[\"ask_size1\"] + df[\"bid_price2\"] * df[\n",
    "        \"bid_size2\"] + df[\"ask_price2\"] * df[\"ask_size2\"]) / (\n",
    "                  df[\"bid_size1\"] + df[\"ask_size1\"] + df[\"bid_size2\"] + df[\"ask_size2\"])\n",
    "    return wp_\n",
    "\n",
    "\n",
    "def maximum_drawdown(series, window=600):\n",
    "    # window for 10 minutes, use min_periods=1 if you want to allow the expanding window\n",
    "    roll_max = series.rolling(window, min_periods=1).max()\n",
    "    second_drawdown = series / roll_max - 1.0\n",
    "    max_drawdown = second_drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff().fillna(0)\n",
    "\n",
    "\n",
    "def rolling_log_return(series, rolling=60):\n",
    "    return np.log(series.rolling(rolling)).diff().fillna(0)\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series ** 2))\n",
    "\n",
    "\n",
    "def diff(series):\n",
    "    return series.diff().fillna(0)\n",
    "\n",
    "\n",
    "def time_diff(series):\n",
    "    return series.diff().fillna(series)\n",
    "\n",
    "\n",
    "def order_flow_imbalance(df, pos=1):\n",
    "    df[\"bid_price{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"bid_price{}\".format(pos)].apply(diff)\n",
    "    df[\"bid_size{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"bid_price{}\".format(pos)].apply(diff)\n",
    "    df[\"bid_order_flow{}\".format(pos)] = df[\"bid_size{}\".format(pos)].copy(deep=True)\n",
    "    df[\"bid_order_flow{}\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] < 0] *= -1\n",
    "    df[\"bid_order_flow{}\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] == 0] = \\\n",
    "        df[\"bid_size{}_diff\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] == 0]\n",
    "\n",
    "    df[\"ask_price{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"ask_price{}\".format(pos)].apply(diff)\n",
    "    df[\"ask_size{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"ask_price{}\".format(pos)].apply(diff)\n",
    "    df[\"ask_order_flow{}\".format(pos)] = df[\"ask_size{}\".format(pos)].copy(deep=True)\n",
    "    df[\"ask_order_flow{}\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] < 0] *= -1\n",
    "    df[\"ask_order_flow{}\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] == 0] = \\\n",
    "        df[\"ask_size{}_diff\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] == 0]\n",
    "\n",
    "    order_flow_imbalance_ = df[\"bid_order_flow{}\".format(pos)] - df[\"ask_order_flow{}\".format(pos)]\n",
    "\n",
    "    df.drop([\"bid_price{}_diff\".format(pos), \"bid_size{}_diff\".format(pos), \"bid_order_flow{}\".format(pos),\n",
    "             \"ask_price{}_diff\".format(pos), \"ask_size{}_diff\".format(pos), \"ask_order_flow{}\".format(pos)], axis=1,\n",
    "            inplace=True)\n",
    "\n",
    "    return order_flow_imbalance_ + 1e-8\n",
    "\n",
    "\n",
    "def order_book_slope(df):\n",
    "\n",
    "    df[\"mid_point\"] = (df[\"bid_price1\"] + df[\"ask_price1\"]) / 2\n",
    "    best_mid_point_ = df.groupby([\"time_id\"])[\"mid_point\"].agg(\"max\").reset_index()\n",
    "    best_mid_point_ = best_mid_point_.rename(columns={\"mid_point\": \"best_mid_point\"})\n",
    "    df = df.merge(best_mid_point_, how=\"left\", on=\"time_id\")\n",
    "\n",
    "    best_mid_point = df[\"best_mid_point\"].copy()\n",
    "    df.drop([\"mid_point\", \"best_mid_point\"], axis=1, inplace=True)\n",
    "\n",
    "    def ratio(series):\n",
    "        ratio_ = series / series.shift()\n",
    "        return ratio_\n",
    "\n",
    "    bid_price1_ratio = df.groupby([\"time_id\"])[\"bid_price1\"].apply(ratio)\n",
    "    bid_price1_mid_point_ratio = df[\"bid_price1\"] / best_mid_point\n",
    "    bid_price1_ratio = abs(bid_price1_ratio.fillna(bid_price1_mid_point_ratio) - 1)\n",
    "\n",
    "    bid_size1_ratio = df.groupby([\"time_id\"])[\"bid_size1\"].apply(ratio) - 1\n",
    "    bid_size1_ratio = bid_size1_ratio.fillna(df[\"bid_size1\"])\n",
    "    df[\"DE\"] = (bid_size1_ratio / bid_price1_ratio).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    ask_price1_ratio = df.groupby([\"time_id\"])[\"ask_price1\"].apply(ratio)\n",
    "    ask_price1_mid_point_ratio = df[\"ask_price1\"] / best_mid_point\n",
    "    ask_price1_ratio = abs(ask_price1_ratio.fillna(ask_price1_mid_point_ratio) - 1)\n",
    "\n",
    "    ask_size1_ratio = df.groupby([\"time_id\"])[\"ask_size1\"].apply(ratio) - 1\n",
    "    ask_size1_ratio = ask_size1_ratio.fillna(df[\"ask_size1\"])\n",
    "    df[\"SE\"] = (ask_size1_ratio / ask_price1_ratio).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    df[\"order_book_slope\"] = (df[\"DE\"] + df[\"SE\"]) / 2\n",
    "    order_book_slope_ = df.groupby([\"time_id\"])[\"order_book_slope\"].agg(\"mean\").reset_index()\n",
    "    df.drop([\"order_book_slope\", \"DE\", \"SE\"], axis=1, inplace=True)\n",
    "\n",
    "    return order_book_slope_\n",
    "\n",
    "\n",
    "def ldispersion(df):\n",
    "    LDispersion = 1 / 2 * (\n",
    "            df[\"bid_size1\"] / (df[\"bid_size1\"] + df[\"bid_size2\"]) * abs(df[\"bid_price1\"] - df[\"bid_price2\"]) + df[\n",
    "        \"ask_size1\"] / (df[\"ask_size1\"] + df[\"ask_size2\"]) * abs(df[\"ask_price1\"] - df[\"ask_price2\"]))\n",
    "    return LDispersion\n",
    "\n",
    "\n",
    "def depth_imbalance(df, pos=1):\n",
    "    depth_imbalance_ = (df[\"bid_size{}\".format(pos)] - df[\"ask_size{}\".format(pos)]) / (\n",
    "            df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "\n",
    "    return depth_imbalance_\n",
    "\n",
    "\n",
    "def height_imbalance(df, pos=1):\n",
    "    height_imbalance_ = (df[\"bid_price{}\".format(pos)] - df[\"ask_price{}\".format(pos)]) / (\n",
    "            df[\"bid_price{}\".format(pos)] + df[\"ask_price{}\".format(pos)])\n",
    "\n",
    "    return height_imbalance_\n",
    "\n",
    "\n",
    "def pressure_imbalance(df):\n",
    "    mid_price = (df[\"bid_price1\"] + df[\"ask_price1\"]) / 2\n",
    "\n",
    "    weight_buy = mid_price / (mid_price - df[\"bid_price1\"]) + mid_price / (mid_price - df[\"bid_price2\"])\n",
    "    pressure_buy = df[\"bid_size1\"] * (mid_price / (mid_price - df[\"bid_price1\"])) / weight_buy + df[\"bid_size2\"] * (\n",
    "            mid_price / (mid_price - df[\"bid_price2\"])) / weight_buy\n",
    "\n",
    "    weight_sell = mid_price / (df[\"ask_price1\"] - mid_price) + mid_price / (df[\"ask_price2\"] - mid_price)\n",
    "    pressure_sell = df[\"ask_size1\"] * (mid_price / (df[\"ask_price1\"] - mid_price)) / weight_sell + df[\"ask_size2\"] * (\n",
    "            mid_price / (df[\"ask_price2\"] - mid_price)) / weight_sell\n",
    "\n",
    "    pressure_imbalance_ = np.log(pressure_buy) - np.log(pressure_sell)\n",
    "\n",
    "    return pressure_imbalance_\n",
    "\n",
    "\n",
    "def relative_spread(df, pos=1):\n",
    "    relative_spread_ = 2 * (df[\"ask_price{}\".format(pos)] - df[\"bid_price{}\".format(pos)]) / (\n",
    "            df[\"ask_price{}\".format(pos)] + df[\"bid_price{}\".format(pos)])\n",
    "\n",
    "    return relative_spread_\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc4c77-114b-44e6-a238-6ac10de336ce",
   "metadata": {},
   "source": [
    "# Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4628652-1114-4c18-a488-71833798085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # float 64 to float 32\n",
    "    float_cols = df.select_dtypes(include=[np.float64]).columns\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "    # int 64 to int 32\n",
    "    int_cols = df.select_dtypes(include=[np.int64]).columns\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "\n",
    "    # Calculate seconds gap\n",
    "    df[\"seconds_gap\"] = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].apply(time_diff)\n",
    "\n",
    "    # Calculate Wap\n",
    "    df[\"wap1\"] = calc_wap(df, pos=1)\n",
    "    df[\"wap2\"] = calc_wap(df, pos=2)\n",
    "\n",
    "    # Calculate wap balance\n",
    "    df[\"wap_balance\"] = abs(df[\"wap1\"] - df[\"wap2\"])\n",
    "\n",
    "    # Calculate log returns\n",
    "    df[\"log_return1\"] = df.groupby([\"time_id\"])[\"wap1\"].apply(log_return)\n",
    "    df[\"log_return2\"] = df.groupby([\"time_id\"])[\"wap2\"].apply(log_return)\n",
    "\n",
    "    # Calculate spread\n",
    "    df[\"bid_ask_spread1\"] = df[\"ask_price1\"] / df[\"bid_price1\"] - 1\n",
    "    df[\"bid_ask_spread2\"] = df[\"ask_price2\"] / df[\"bid_price2\"] - 1\n",
    "\n",
    "    # order flow imbalance\n",
    "    df[\"order_flow_imbalance1\"] = order_flow_imbalance(df, 1)\n",
    "    df[\"order_flow_imbalance2\"] = order_flow_imbalance(df, 2)\n",
    "\n",
    "    # order book slope\n",
    "    order_slope_ = order_book_slope(df)\n",
    "    df = df.merge(order_slope_, how=\"left\", on=\"time_id\")\n",
    "\n",
    "    # depth imbalance\n",
    "    df[\"depth_imbalance1\"] = depth_imbalance(df, pos=1)\n",
    "    df[\"depth_imbalance2\"] = depth_imbalance(df, pos=2)\n",
    "\n",
    "    # height imbalance\n",
    "    df[\"height_imbalance1\"] = height_imbalance(df, pos=1)\n",
    "    df[\"height_imbalance2\"] = height_imbalance(df, pos=2)\n",
    "\n",
    "    # pressure imbalance\n",
    "    df[\"pressure_imbalance\"] = pressure_imbalance(df)\n",
    "\n",
    "    # total volume\n",
    "    df[\"total_volume\"] = (df[\"ask_size1\"] + df[\"ask_size2\"]) + (df[\"bid_size1\"] + df[\"bid_size2\"])\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        \"wap1\": [np.sum, np.std],\n",
    "        \"wap2\": [np.sum, np.std],\n",
    "        \"log_return1\": [realized_volatility],\n",
    "        \"log_return2\": [realized_volatility],\n",
    "        \"wap_balance\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_book_slope\": [np.mean, np.max],\n",
    "        \"depth_imbalance1\": [np.sum, np.max, np.std],\n",
    "        \"depth_imbalance2\": [np.sum, np.max, np.std],\n",
    "        \"height_imbalance1\": [np.sum, np.max, np.std],\n",
    "        \"height_imbalance2\": [np.sum, np.max, np.std],\n",
    "        \"pressure_imbalance\": [np.sum, np.max, np.std],\n",
    "        \"total_volume\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        \"log_return1\": [realized_volatility],\n",
    "        \"log_return2\": [realized_volatility],\n",
    "        \"wap_balance\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"total_volume\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(feature_dict, seconds_in_bucket, add_suffix=False):\n",
    "        # Group by the window\n",
    "        df_feature_ = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(\n",
    "            feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature_.columns = [\"_\".join(col) for col in df_feature_.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature_ = df_feature_.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "        return df_feature_\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    windows = [0, 150, 300, 450]\n",
    "    add_suffixes = [False, True, True, True]\n",
    "    df_feature = None\n",
    "\n",
    "    for window, add_suffix in zip(windows, add_suffixes):\n",
    "        if df_feature is None:\n",
    "            df_feature = get_stats_window(feature_dict=create_feature_dict, seconds_in_bucket=window,\n",
    "                                          add_suffix=add_suffix)\n",
    "        else:\n",
    "            new_df_feature = get_stats_window(feature_dict=create_feature_dict_time, seconds_in_bucket=window,\n",
    "                                              add_suffix=add_suffix)\n",
    "            df_feature = df_feature.merge(new_df_feature, how=\"left\", left_on=\"time_id_\",\n",
    "                                          right_on=\"time_id__{}\".format(window))\n",
    "\n",
    "            # Drop unnecesary time_ids\n",
    "            df_feature.drop([\"time_id__{}\".format(window)], axis=1, inplace=True)\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature.drop([\"time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97fcef12-2b0b-477f-9e24-ad8eadc4c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # float 64 to float 32\n",
    "    float_cols = df.select_dtypes(include=[np.float64]).columns\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "    # int 64 to int 32\n",
    "    int_cols = df.select_dtypes(include=[np.int64]).columns\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "\n",
    "    # Calculate seconds gap\n",
    "    df[\"seconds_gap\"] = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].apply(time_diff)\n",
    "\n",
    "    # Calculate log return\n",
    "    df[\"price_log_return\"] = df.groupby(\"time_id\")[\"price\"].apply(log_return)\n",
    "\n",
    "    # Calculate volumes\n",
    "    df[\"volumes\"] = df[\"price\"] * df[\"size\"]\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        \"price_log_return\": [realized_volatility],\n",
    "        \"volumes\": [np.sum, np.max, np.std],\n",
    "        \"order_count\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        \"price_log_return\": [realized_volatility],\n",
    "        \"volumes\": [np.sum, np.max, np.std],\n",
    "        \"order_count\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(feature_dict, seconds_in_bucket, add_suffix=False):\n",
    "        # Group by the window\n",
    "        df_feature_ = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(\n",
    "            feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature_.columns = [\"_\".join(col) for col in df_feature_.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature_ = df_feature_.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "        return df_feature_\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    windows = [0, 150, 300, 450]\n",
    "    add_suffixes = [False, True, True, True]\n",
    "    df_feature = None\n",
    "\n",
    "    for window, add_suffix in zip(windows, add_suffixes):\n",
    "        if df_feature is None:\n",
    "            df_feature = get_stats_window(feature_dict=create_feature_dict, seconds_in_bucket=window,\n",
    "                                          add_suffix=add_suffix)\n",
    "        else:\n",
    "            new_df_feature = get_stats_window(feature_dict=create_feature_dict_time, seconds_in_bucket=window,\n",
    "                                              add_suffix=add_suffix)\n",
    "            df_feature = df_feature.merge(new_df_feature, how=\"left\", left_on=\"time_id_\",\n",
    "                                          right_on=\"time_id__{}\".format(window))\n",
    "\n",
    "            # Drop unnecesary time_ids\n",
    "            df_feature.drop([\"time_id__{}\".format(window)], axis=1, inplace=True)\n",
    "\n",
    "    def tendency(price, vol):\n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff / price[1:]) * 100\n",
    "        power = np.sum(val * vol[1:])\n",
    "        return (power)\n",
    "\n",
    "    lis = []\n",
    "    for n_time_id in df[\"time_id\"].unique():\n",
    "        \n",
    "        df_id = df[df[\"time_id\"] == n_time_id]\n",
    "        \n",
    "        tendencyV = tendency(df_id[\"price\"].values, df_id[\"size\"].values)\n",
    "        energy = np.mean(df_id[\"price\"].values ** 2)\n",
    "\n",
    "        lis.append(\n",
    "            {\n",
    "                \"time_id\": n_time_id,\n",
    "                \"tendency\": tendencyV,\n",
    "                \"energy\": energy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_feature = df_feature.merge(df_lr, how=\"left\", left_on=\"time_id_\", right_on=\"time_id\")\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    df_feature = df_feature.add_prefix(\"trade_\")\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"trade_time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature.drop([\"trade_time_id_\", \"trade_time_id\"], axis=1, inplace=True)\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2dadda6-499a-4969-bef7-2d47d8178ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = CONFIG[\"root_dir\"] + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = CONFIG[\"root_dir\"] + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = CONFIG[\"root_dir\"] + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = CONFIG[\"root_dir\"] + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = \"row_id\", how = \"left\")\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    \n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2a4b5d-63aa-4992-97b6-a3df4e43f1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  4.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train[\"stock_id\"].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train=True)\n",
    "train = train.merge(train_, on=[\"row_id\"], how=\"left\")\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test[\"stock_id\"].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train=False)\n",
    "test = test.merge(test_, on=[\"row_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278df512-5e94-40a6-b9da-2ad0350c1346",
   "metadata": {},
   "source": [
    "# Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e56f947-264b-4a2f-83f9-4d4ec06ad913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs log columns\n",
    "abs_log_columns = [column for column in train.columns if \n",
    "                       \"order_flow_imbalance\" in column or \n",
    "                       \"order_book_slope\" in column or \n",
    "                       \"depth_imbalance\" in column or \n",
    "                       \"pressure_imbalance\" in column or\n",
    "                       \"total_volume\" in column or\n",
    "                       \"seconds_gap\" in column or\n",
    "                       \"trade_volumes\" in column or\n",
    "                       \"trade_order_count\" in column or\n",
    "                       \"trade_seconds_gap\" in column or\n",
    "                       \"trade_tendency\" in column\n",
    "                      ]\n",
    "\n",
    "# apply abs + 1e-8 + log\n",
    "train[abs_log_columns] = (train[abs_log_columns].apply(np.abs) + 1e-8).apply(np.log)\n",
    "test[abs_log_columns] = (test[abs_log_columns].apply(np.abs) + 1e-8).apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829b635-1244-4389-8f98-f7a5f9114ab3",
   "metadata": {},
   "source": [
    "# Fill inf with nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a02e48-a788-4572-a1ce-d5b554944430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "train = train.fillna(train.mean())\n",
    "test = test.fillna(train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8263237-9f5a-4790-b491-1cfb9636a379",
   "metadata": {},
   "source": [
    "# Agg features inside fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64e7584-8a75-4192-b88f-19ccb01653fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process agg by kmeans\n",
    "def get_kmeans_idx(n_clusters=7):\n",
    "    train_p = pd.read_csv(\"../../input/optiver-realized-volatility-prediction/train.csv\")\n",
    "    train_p = train_p.pivot(index=\"time_id\", columns=\"stock_id\", values=\"target\")\n",
    "\n",
    "    corr = train_p.corr()\n",
    "\n",
    "    ids = corr.index\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(corr.values)\n",
    "\n",
    "    kmeans_clusters = []\n",
    "    for n in range(n_clusters):\n",
    "        kmeans_clusters.append ([(x - 1) for x in ((ids + 1)*(kmeans.labels_ == n)) if x > 0])\n",
    "        \n",
    "    return kmeans_clusters\n",
    "    \n",
    "\n",
    "def agg_stat_features_by_clusters(df, n_clusters=7, function=np.nanmean, post_fix=\"_cluster_mean\"):\n",
    "\n",
    "    kmeans_clusters = get_kmeans_idx(n_clusters=n_clusters)\n",
    "\n",
    "    clusters = []\n",
    "    agg_columns = [\n",
    "        \"time_id\",\n",
    "        \"stock_id\",\n",
    "        \"log_return1_realized_volatility\",\n",
    "        \"log_return2_realized_volatility\",\n",
    "        \"order_flow_imbalance1_sum\",\n",
    "        \"order_flow_imbalance2_sum\",\n",
    "        \"order_book_slope_mean\",\n",
    "        \"depth_imbalance1_std\",\n",
    "        \"depth_imbalance2_std\",\n",
    "        \"height_imbalance1_sum\",\n",
    "        \"height_imbalance2_sum\",\n",
    "        \"pressure_imbalance_std\",\n",
    "        \"total_volume_sum\",\n",
    "        \"seconds_gap_mean\",\n",
    "        \"trade_price_log_return_realized_volatility\",\n",
    "        \"trade_volumes_sum\",\n",
    "        \"trade_order_count_sum\",\n",
    "        \"trade_seconds_gap_mean\",\n",
    "        \"trade_tendency\",\n",
    "        \"trade_energy\"\n",
    "    ]\n",
    "\n",
    "    for cluster_idx, ind in enumerate(kmeans_clusters):\n",
    "        cluster_df = df.loc[df[\"stock_id\"].isin(ind), agg_columns].groupby([\"time_id\"]).agg(function)\n",
    "        cluster_df.loc[:, \"stock_id\"] = str(cluster_idx) + post_fix\n",
    "        clusters.append(cluster_df)\n",
    "\n",
    "    clusters_df = pd.concat(clusters).reset_index()\n",
    "    # multi index (column, c1)\n",
    "    clusters_df = clusters_df.pivot(index=\"time_id\", columns=\"stock_id\")\n",
    "    # ravel multi index to list of tuple [(target, c1), ...]\n",
    "    clusters_df.columns = [\"_\".join(x) for x in clusters_df.columns.ravel()]\n",
    "    clusters_df.reset_index(inplace=True)\n",
    "\n",
    "    postfixes = [\n",
    "        \"0\" + post_fix,\n",
    "        \"1\" + post_fix,\n",
    "        \"3\" + post_fix,\n",
    "        \"4\" + post_fix,\n",
    "        \"6\" + post_fix,\n",
    "    ]\n",
    "    merge_columns = []\n",
    "    for column in agg_columns:\n",
    "        if column == \"time_id\":\n",
    "            merge_columns.append(column)\n",
    "        elif column == \"stock_id\":\n",
    "            continue\n",
    "        else:\n",
    "            for postfix in postfixes:\n",
    "                merge_columns.append(column + \"_\" + postfix)\n",
    "    df = pd.merge(df, clusters_df[merge_columns], how=\"left\", on=\"time_id\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to get group stats for the time_id\n",
    "def agg_stat_features_by_market(df, operations=None, operations_names=None):\n",
    "    def percentile(n):\n",
    "        def percentile_(x):\n",
    "            return np.percentile(x, n)\n",
    "\n",
    "        percentile_.__name__ = \"percentile_%s\" % n\n",
    "        return percentile_\n",
    "\n",
    "    if operations is None:\n",
    "        operations = [\n",
    "            np.nanmean,\n",
    "        ]\n",
    "        operations_names = [\n",
    "            \"mean\",\n",
    "        ]\n",
    "\n",
    "    # Get realized volatility columns\n",
    "    vol_cols = [\n",
    "        \"log_return1_realized_volatility\",\n",
    "        \"log_return1_realized_volatility_150\",\n",
    "        \"log_return1_realized_volatility_300\",\n",
    "        \"log_return1_realized_volatility_450\",\n",
    "    ]\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby([\"stock_id\"])[vol_cols].agg(operations).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = [\"_\".join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix(\"_\" + \"stock\")\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby([\"time_id\"])[vol_cols].agg(operations).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = [\"_\".join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix(\"_\" + \"time\")\n",
    "\n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how=\"left\", left_on=[\"stock_id\"], right_on=[\"stock_id__stock\"])\n",
    "    df.drop(\"stock_id__stock\", axis=1, inplace=True)\n",
    "\n",
    "    df = df.merge(df_time_id, how=\"left\", left_on=[\"time_id\"], right_on=[\"time_id__time\"])\n",
    "    df.drop(\"time_id__time\", axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a95433-524b-4d66-a6d2-3208e3449ed2",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "398a9da6-4930-4da3-8bc6-5a9b210b062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        \n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "    \n",
    "\n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8775544d-92b2-44b3-9f66-8b2ec8422748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test):\n",
    "    \n",
    "    # label encoder\n",
    "    cat_columns = [\"stock_id\"]\n",
    "    label_encoder = LabelEncoder()\n",
    "    train[cat_columns] = label_encoder.fit_transform(train[cat_columns].values)\n",
    "    test[cat_columns] = label_encoder.transform(test[cat_columns].values)\n",
    "    cat_dims = [len(label_encoder.classes_)]\n",
    "    \n",
    "    # scaler\n",
    "    # scaler = QuantileTransformer(n_quantiles=2000, random_state=2021)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Split features and target\n",
    "    x = train.drop([\"row_id\", \"target\"], axis=1)\n",
    "    y = train[\"target\"]\n",
    "    \n",
    "    # x_test with train feature\n",
    "    x_test = test.drop(\"row_id\", axis=1)\n",
    "    x_test = agg_stat_features_by_market(x_test)\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean,\n",
    "                                           post_fix=\"_cluster_mean\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax,\n",
    "                                           post_fix=\"_cluster_max\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin,\n",
    "                                           post_fix=\"_cluster_min\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd,\n",
    "                                           post_fix=\"_cluster_std\")\n",
    "\n",
    "    # define normalize columns\n",
    "    except_columns = [\"stock_id\", \"time_id\", \"target\", \"row_id\"]\n",
    "    normalized_columns = [column for column in x_test.columns if column not in except_columns]\n",
    "    x_test.drop(\"time_id\", axis=1, inplace=True)\n",
    "    \n",
    "    # Process categorical features and get params dict\n",
    "    cat_idxs = [i for i, f in enumerate(x_test.columns.tolist()) if f in cat_columns]\n",
    "    \n",
    "    params = dict(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        cat_emb_dim=1,\n",
    "        n_d=16,\n",
    "        n_a=16,\n",
    "        n_steps=2,\n",
    "        gamma=2,\n",
    "        n_independent=2,\n",
    "        n_shared=2,\n",
    "        lambda_sparse=0,\n",
    "        optimizer_fn=Adam,\n",
    "        optimizer_params=dict(lr = (2e-2)),\n",
    "        mask_type=\"entmax\",\n",
    "        scheduler_params=dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "        scheduler_fn=CosineAnnealingWarmRestarts,\n",
    "        seed=42,\n",
    "        verbose=10\n",
    "    )\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    \n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    \n",
    "    # Statistics\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances[\"feature\"] = x_test.columns.tolist()\n",
    "    stats = pd.DataFrame()\n",
    "    explain_matrices = []\n",
    "    masks_ =[]\n",
    "    \n",
    "    # Create a KFold object\n",
    "    kfold = GroupKFold(n_splits=CONFIG[\"n_splits\"])\n",
    "    x_ref = x.copy(deep=True)\n",
    "    x_ref = agg_stat_features_by_market(x_ref)\n",
    "    x_ref = agg_stat_features_by_clusters(x_ref, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "    x_ref = agg_stat_features_by_clusters(x_ref, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "    x_ref = agg_stat_features_by_clusters(x_ref, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "    x_ref = agg_stat_features_by_clusters(x_ref, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "    x_ref[normalized_columns] = scaler.fit_transform(x_ref[normalized_columns])\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x, groups=x[\"time_id\"])):\n",
    "        \n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        x_train = x.iloc[trn_ind]\n",
    "        x_train = agg_stat_features_by_market(x_train)\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean,\n",
    "                                                post_fix=\"_cluster_mean\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax,\n",
    "                                                post_fix=\"_cluster_max\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin,\n",
    "                                                post_fix=\"_cluster_min\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd,\n",
    "                                                post_fix=\"_cluster_std\")\n",
    "        x_train.drop(\"time_id\", axis=1, inplace=True)\n",
    "        scaler = scaler.fit(x_train[normalized_columns])\n",
    "        dump(scaler, os.path.join(CONFIG[\"ckpt_dir\"], \"std_scaler_fold_{}.bin\".format(fold + 1)), compress=True)\n",
    "        x_train[normalized_columns] = scaler.transform(x_train[normalized_columns])\n",
    "\n",
    "        x_val = x.iloc[val_ind]\n",
    "        x_val = agg_stat_features_by_market(x_val)\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean,\n",
    "                                              post_fix=\"_cluster_mean\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax,\n",
    "                                              post_fix=\"_cluster_max\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin,\n",
    "                                              post_fix=\"_cluster_min\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd,\n",
    "                                              post_fix=\"_cluster_std\")\n",
    "        x_val.drop(\"time_id\", axis=1, inplace=True)\n",
    "        x_val[normalized_columns] = scaler.transform(x_val[normalized_columns])\n",
    "        \n",
    "        y_train, y_val = y.iloc[trn_ind].values.reshape(-1, 1), y.iloc[val_ind].values.reshape(-1, 1)\n",
    "        \n",
    "        # Train\n",
    "        clf =  TabNetRegressor(**params)\n",
    "        clf.fit(\n",
    "              x_train.values, y_train,\n",
    "              eval_set=[(x_val.values, y_val)],\n",
    "              max_epochs=200,\n",
    "              patience=50,\n",
    "              batch_size=1024*20, \n",
    "              virtual_batch_size=128*20,\n",
    "              num_workers=0,\n",
    "              drop_last=False,\n",
    "              eval_metric=[RMSPE],\n",
    "              loss_fn=RMSPELoss\n",
    "          )\n",
    "        \n",
    "        # save model\n",
    "        saved_filepath = clf.save_model(os.path.join(CONFIG[\"ckpt_path\"], \"tabnet_fold{}\".format(fold + 1)))\n",
    "        \n",
    "        # save statistics\n",
    "        explain_matrix, masks = clf.explain(x_val.values)\n",
    "        explain_matrices.append(explain_matrix)\n",
    "        masks_.append(masks[0])\n",
    "        masks_.append(masks[1])\n",
    "        \n",
    "        feature_importances[\"importance_fold{}\".format(fold + 1)] = clf.feature_importances_\n",
    "        stats[\"fold{}_train_rmspe\".format(fold + 1)] = clf.history[\"loss\"]\n",
    "        stats[\"fold{}_val_rmspe\".format(fold + 1)] = clf.history[\"val_0_rmspe\"]\n",
    "        \n",
    "        # save oof and test predictions\n",
    "        oof_predictions[val_ind] = clf.predict(x_val.values).flatten()\n",
    "        x_test_ = x_test.copy()\n",
    "        x_test_[normalized_columns] = scaler.transform(x_test_[normalized_columns])\n",
    "        test_predictions += clf.predict(x_test_.values).flatten() / CONFIG[\"n_splits\"]\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(\"Our out of folds RMSPE is {}\".format(rmspe_score))\n",
    "    \n",
    "    # Return test predictions\n",
    "    return test_predictions, stats, feature_importances, explain_matrices, masks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15fb8ffa-a102-402c-82b6-aedf895d02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 149.2314| val_0_rmspe: 24.20727|  0:00:09s\n",
      "epoch 10 | loss: 0.57882 | val_0_rmspe: 0.65378 |  0:01:33s\n",
      "epoch 20 | loss: 0.36284 | val_0_rmspe: 0.31292 |  0:02:58s\n",
      "epoch 30 | loss: 0.27479 | val_0_rmspe: 0.26914 |  0:04:22s\n",
      "epoch 40 | loss: 0.27852 | val_0_rmspe: 0.27144 |  0:05:47s\n",
      "epoch 50 | loss: 0.26156 | val_0_rmspe: 0.28282 |  0:07:11s\n",
      "epoch 60 | loss: 0.25614 | val_0_rmspe: 0.26375 |  0:08:35s\n",
      "epoch 70 | loss: 0.22692 | val_0_rmspe: 0.23386 |  0:09:59s\n",
      "epoch 80 | loss: 0.22704 | val_0_rmspe: 0.22848 |  0:11:24s\n",
      "epoch 90 | loss: 0.22407 | val_0_rmspe: 0.23075 |  0:12:48s\n",
      "epoch 100| loss: 0.21599 | val_0_rmspe: 0.22683 |  0:14:12s\n",
      "epoch 110| loss: 0.20966 | val_0_rmspe: 0.23285 |  0:15:36s\n",
      "epoch 120| loss: 0.20448 | val_0_rmspe: 0.22592 |  0:17:00s\n",
      "epoch 130| loss: 0.20038 | val_0_rmspe: 0.22617 |  0:18:24s\n",
      "epoch 140| loss: 0.19598 | val_0_rmspe: 0.22736 |  0:19:47s\n",
      "epoch 150| loss: 0.19358 | val_0_rmspe: 0.22963 |  0:21:11s\n",
      "epoch 160| loss: 0.19188 | val_0_rmspe: 0.23008 |  0:22:35s\n",
      "\n",
      "Early stopping occurred at epoch 165 with best_epoch = 115 and best_val_0_rmspe = 0.2245\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e8fa6646ed67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Traing and evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_importances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplain_matrices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Save test predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# test[\"target\"] = test_predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-e9f66a6a8844>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# save oof and test predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0moof_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_ind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mtest_predictions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"n_splits\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mrmspe_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrmspe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moof_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtabnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_masks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m         \u001b[0msteps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, prior)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;31m# output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mmasked_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeat_transformers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_d\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0msteps_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecifics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\tab_network.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m         \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# the first layer of the block has no scale multiplication\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglu_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "test_predictions, stats, feature_importances, explain_matrices, masks_ = train_and_evaluate(train, test)\n",
    "\n",
    "# Save test predictions\n",
    "# test[\"target\"] = test_predictions\n",
    "# test[[\"row_id\", \"target\"]].to_csv(\"submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172554d-d015-404d-b3be-edde8ab5bd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "trade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
