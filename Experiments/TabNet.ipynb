{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7fbf87-8399-4b5a-90cd-8ce5e3ad9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "import pandas as pd\n",
    "from pandas.core.common import flatten\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"max_columns\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92f4e0-97b5-4951-bb7c-f3ff176a09c4",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48612360-2cb2-47d3-8e47-836f81121284",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"root_dir\": \"../../input/optiver-realized-volatility-prediction/\",\n",
    "    \"ckpt_path\": \"../../ckpts/\",\n",
    "    \"kfold_seed\": 42,\n",
    "    \"n_splits\": 5,\n",
    "    \"n_clusters\": 7,\n",
    "    \"shuffle\": True,\n",
    "    \"shuffle_seed\": 1997,\n",
    "    \"pretrained\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db39f3d-aaa7-4144-9182-46906cb8880e",
   "metadata": {},
   "source": [
    "# Read Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6aa2bb0-c3f5-46cd-bd8d-ceba27b8903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test():\n",
    "    \n",
    "    train = pd.read_csv(\"../../input/optiver-realized-volatility-prediction/train.csv\")\n",
    "    test = pd.read_csv(\"../../input/optiver-realized-volatility-prediction/test.csv\")\n",
    "    \n",
    "    # Create a key to merge with book and trade data\n",
    "    train[\"row_id\"] = train[\"stock_id\"].astype(str) + \"-\" + train[\"time_id\"].astype(str)\n",
    "    test[\"row_id\"] = test[\"stock_id\"].astype(str) + \"-\" + test[\"time_id\"].astype(str)\n",
    "    \n",
    "    print(\"Our training set has {} rows\".format(train.shape[0]))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0382e-681a-478d-b7d5-ed6e6963c708",
   "metadata": {},
   "source": [
    "# Define basic metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3eea82-a842-4980-ba01-05850f8f29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_counts(df):\n",
    "    activity_counts_ = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].agg(\"count\").reset_index()\n",
    "    activity_counts_ = activity_counts_.rename(columns={\"seconds_in_bucket\": \"activity_counts\"})\n",
    "    return activity_counts_\n",
    "\n",
    "\n",
    "def calc_wap(df, pos=1):\n",
    "    wap = (df[\"bid_price{}\".format(pos)] * df[\"ask_size{}\".format(pos)] + df[\"ask_price{}\".format(pos)] * df[\n",
    "        \"bid_size{}\".format(pos)]) / (df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def calc_wap2(df, pos=1):\n",
    "    wap = (df[\"bid_price{}\".format(pos)] * df[\"bid_size{}\".format(pos)] + df[\"ask_price{}\".format(pos)] * df[\n",
    "        \"ask_size{}\".format(pos)]) / (df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "    return wap\n",
    "\n",
    "\n",
    "def wp(df):\n",
    "    wp_ = (df[\"bid_price1\"] * df[\"bid_size1\"] + df[\"ask_price1\"] * df[\"ask_size1\"] + df[\"bid_price2\"] * df[\n",
    "        \"bid_size2\"] + df[\"ask_price2\"] * df[\"ask_size2\"]) / (\n",
    "                  df[\"bid_size1\"] + df[\"ask_size1\"] + df[\"bid_size2\"] + df[\"ask_size2\"])\n",
    "    return wp_\n",
    "\n",
    "\n",
    "def maximum_drawdown(series, window=600):\n",
    "    # window for 10 minutes, use min_periods=1 if you want to allow the expanding window\n",
    "    roll_max = series.rolling(window, min_periods=1).max()\n",
    "    second_drawdown = series / roll_max - 1.0\n",
    "    max_drawdown = second_drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "def log_return(series):\n",
    "    return np.log(series).diff().fillna(0)\n",
    "\n",
    "\n",
    "def rolling_log_return(series, rolling=60):\n",
    "    return np.log(series.rolling(rolling)).diff().fillna(0)\n",
    "\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series ** 2))\n",
    "\n",
    "\n",
    "def diff(series):\n",
    "    return series.diff().fillna(0)\n",
    "\n",
    "\n",
    "def time_diff(series):\n",
    "    return series.diff().fillna(series)\n",
    "\n",
    "\n",
    "def order_flow_imbalance(df, pos=1):\n",
    "    df[\"bid_price{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"bid_price{}\".format(pos)].apply(diff)\n",
    "    df[\"bid_size{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"bid_price{}\".format(pos)].apply(diff)\n",
    "    df[\"bid_order_flow{}\".format(pos)] = df[\"bid_size{}\".format(pos)].copy(deep=True)\n",
    "    df[\"bid_order_flow{}\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] < 0] *= -1\n",
    "    df[\"bid_order_flow{}\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] == 0] = \\\n",
    "        df[\"bid_size{}_diff\".format(pos)].loc[df[\"bid_price{}_diff\".format(pos)] == 0]\n",
    "\n",
    "    df[\"ask_price{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"ask_price{}\".format(pos)].apply(diff)\n",
    "    df[\"ask_size{}_diff\".format(pos)] = df.groupby([\"time_id\"])[\"ask_price{}\".format(pos)].apply(diff)\n",
    "    df[\"ask_order_flow{}\".format(pos)] = df[\"ask_size{}\".format(pos)].copy(deep=True)\n",
    "    df[\"ask_order_flow{}\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] < 0] *= -1\n",
    "    df[\"ask_order_flow{}\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] == 0] = \\\n",
    "        df[\"ask_size{}_diff\".format(pos)].loc[df[\"ask_price{}_diff\".format(pos)] == 0]\n",
    "\n",
    "    order_flow_imbalance_ = df[\"bid_order_flow{}\".format(pos)] - df[\"ask_order_flow{}\".format(pos)]\n",
    "\n",
    "    df.drop([\"bid_price{}_diff\".format(pos), \"bid_size{}_diff\".format(pos), \"bid_order_flow{}\".format(pos),\n",
    "             \"ask_price{}_diff\".format(pos), \"ask_size{}_diff\".format(pos), \"ask_order_flow{}\".format(pos)], axis=1,\n",
    "            inplace=True)\n",
    "\n",
    "    return order_flow_imbalance_ + 1e-8\n",
    "\n",
    "\n",
    "def order_book_slope(df):\n",
    "\n",
    "    df[\"mid_point\"] = (df[\"bid_price1\"] + df[\"ask_price1\"]) / 2\n",
    "    best_mid_point_ = df.groupby([\"time_id\"])[\"mid_point\"].agg(\"max\").reset_index()\n",
    "    best_mid_point_ = best_mid_point_.rename(columns={\"mid_point\": \"best_mid_point\"})\n",
    "    df = df.merge(best_mid_point_, how=\"left\", on=\"time_id\")\n",
    "\n",
    "    best_mid_point = df[\"best_mid_point\"].copy()\n",
    "    df.drop([\"mid_point\", \"best_mid_point\"], axis=1, inplace=True)\n",
    "\n",
    "    def ratio(series):\n",
    "        ratio_ = series / series.shift()\n",
    "        return ratio_\n",
    "\n",
    "    bid_price1_ratio = df.groupby([\"time_id\"])[\"bid_price1\"].apply(ratio)\n",
    "    bid_price1_mid_point_ratio = df[\"bid_price1\"] / best_mid_point\n",
    "    bid_price1_ratio = abs(bid_price1_ratio.fillna(bid_price1_mid_point_ratio) - 1)\n",
    "\n",
    "    bid_size1_ratio = df.groupby([\"time_id\"])[\"bid_size1\"].apply(ratio) - 1\n",
    "    bid_size1_ratio = bid_size1_ratio.fillna(df[\"bid_size1\"])\n",
    "    df[\"DE\"] = (bid_size1_ratio / bid_price1_ratio).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    ask_price1_ratio = df.groupby([\"time_id\"])[\"ask_price1\"].apply(ratio)\n",
    "    ask_price1_mid_point_ratio = df[\"ask_price1\"] / best_mid_point\n",
    "    ask_price1_ratio = abs(ask_price1_ratio.fillna(ask_price1_mid_point_ratio) - 1)\n",
    "\n",
    "    ask_size1_ratio = df.groupby([\"time_id\"])[\"ask_size1\"].apply(ratio) - 1\n",
    "    ask_size1_ratio = ask_size1_ratio.fillna(df[\"ask_size1\"])\n",
    "    df[\"SE\"] = (ask_size1_ratio / ask_price1_ratio).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    df[\"order_book_slope\"] = (df[\"DE\"] + df[\"SE\"]) / 2\n",
    "    order_book_slope_ = df.groupby([\"time_id\"])[\"order_book_slope\"].agg(\"mean\").reset_index()\n",
    "    df.drop([\"order_book_slope\", \"DE\", \"SE\"], axis=1, inplace=True)\n",
    "\n",
    "    return order_book_slope_\n",
    "\n",
    "\n",
    "def ldispersion(df):\n",
    "    LDispersion = 1 / 2 * (\n",
    "            df[\"bid_size1\"] / (df[\"bid_size1\"] + df[\"bid_size2\"]) * abs(df[\"bid_price1\"] - df[\"bid_price2\"]) + df[\n",
    "        \"ask_size1\"] / (df[\"ask_size1\"] + df[\"ask_size2\"]) * abs(df[\"ask_price1\"] - df[\"ask_price2\"]))\n",
    "    return LDispersion\n",
    "\n",
    "\n",
    "def depth_imbalance(df, pos=1):\n",
    "    depth_imbalance_ = (df[\"bid_size{}\".format(pos)] - df[\"ask_size{}\".format(pos)]) / (\n",
    "            df[\"bid_size{}\".format(pos)] + df[\"ask_size{}\".format(pos)])\n",
    "\n",
    "    return depth_imbalance_\n",
    "\n",
    "\n",
    "def height_imbalance(df, pos=1):\n",
    "    height_imbalance_ = (df[\"bid_price{}\".format(pos)] - df[\"ask_price{}\".format(pos)]) / (\n",
    "            df[\"bid_price{}\".format(pos)] + df[\"ask_price{}\".format(pos)])\n",
    "\n",
    "    return height_imbalance_\n",
    "\n",
    "\n",
    "def pressure_imbalance(df):\n",
    "    mid_price = (df[\"bid_price1\"] + df[\"ask_price1\"]) / 2\n",
    "\n",
    "    weight_buy = mid_price / (mid_price - df[\"bid_price1\"]) + mid_price / (mid_price - df[\"bid_price2\"])\n",
    "    pressure_buy = df[\"bid_size1\"] * (mid_price / (mid_price - df[\"bid_price1\"])) / weight_buy + df[\"bid_size2\"] * (\n",
    "            mid_price / (mid_price - df[\"bid_price2\"])) / weight_buy\n",
    "\n",
    "    weight_sell = mid_price / (df[\"ask_price1\"] - mid_price) + mid_price / (df[\"ask_price2\"] - mid_price)\n",
    "    pressure_sell = df[\"ask_size1\"] * (mid_price / (df[\"ask_price1\"] - mid_price)) / weight_sell + df[\"ask_size2\"] * (\n",
    "            mid_price / (df[\"ask_price2\"] - mid_price)) / weight_sell\n",
    "\n",
    "    pressure_imbalance_ = np.log(pressure_buy) - np.log(pressure_sell)\n",
    "\n",
    "    return pressure_imbalance_\n",
    "\n",
    "\n",
    "def relative_spread(df, pos=1):\n",
    "    relative_spread_ = 2 * (df[\"ask_price{}\".format(pos)] - df[\"bid_price{}\".format(pos)]) / (\n",
    "            df[\"ask_price{}\".format(pos)] + df[\"bid_price{}\".format(pos)])\n",
    "\n",
    "    return relative_spread_\n",
    "\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc4c77-114b-44e6-a238-6ac10de336ce",
   "metadata": {},
   "source": [
    "# Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4628652-1114-4c18-a488-71833798085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # float 64 to float 32\n",
    "    float_cols = df.select_dtypes(include=[np.float64]).columns\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "    # int 64 to int 32\n",
    "    int_cols = df.select_dtypes(include=[np.int64]).columns\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    # rebase seconds_in_bucket\n",
    "    df[\"seconds_in_bucket\"] = df[\"seconds_in_bucket\"] - df[\"seconds_in_bucket\"].min()\n",
    "\n",
    "    # Calculate seconds gap\n",
    "    df[\"seconds_gap\"] = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].apply(time_diff)\n",
    "\n",
    "    # Calculate Wap\n",
    "    df[\"wap1\"] = calc_wap(df, pos=1)\n",
    "    df[\"wap2\"] = calc_wap(df, pos=2)\n",
    "\n",
    "    # Calculate wap balance\n",
    "    df[\"wap_balance\"] = abs(df[\"wap1\"] - df[\"wap2\"])\n",
    "\n",
    "    # Calculate log returns\n",
    "    df[\"log_return1\"] = df.groupby([\"time_id\"])[\"wap1\"].apply(log_return)\n",
    "    df[\"log_return2\"] = df.groupby([\"time_id\"])[\"wap2\"].apply(log_return)\n",
    "\n",
    "    # Calculate spread\n",
    "    df[\"bid_ask_spread1\"] = df[\"ask_price1\"] / df[\"bid_price1\"] - 1\n",
    "    df[\"bid_ask_spread2\"] = df[\"ask_price2\"] / df[\"bid_price2\"] - 1\n",
    "\n",
    "    # order flow imbalance\n",
    "    df[\"order_flow_imbalance1\"] = order_flow_imbalance(df, 1)\n",
    "    df[\"order_flow_imbalance2\"] = order_flow_imbalance(df, 2)\n",
    "\n",
    "    # order book slope\n",
    "    order_slope_ = order_book_slope(df)\n",
    "    df = df.merge(order_slope_, how=\"left\", on=\"time_id\")\n",
    "\n",
    "    # depth imbalance\n",
    "    df[\"depth_imbalance1\"] = depth_imbalance(df, pos=1)\n",
    "    df[\"depth_imbalance2\"] = depth_imbalance(df, pos=2)\n",
    "\n",
    "    # height imbalance\n",
    "    df[\"height_imbalance1\"] = height_imbalance(df, pos=1)\n",
    "    df[\"height_imbalance2\"] = height_imbalance(df, pos=2)\n",
    "\n",
    "    # pressure imbalance\n",
    "    df[\"pressure_imbalance\"] = pressure_imbalance(df)\n",
    "\n",
    "    # total volume\n",
    "    df[\"total_volume\"] = (df[\"ask_size1\"] + df[\"ask_size2\"]) + (df[\"bid_size1\"] + df[\"bid_size2\"])\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        \"wap1\": [np.sum, np.std],\n",
    "        \"wap2\": [np.sum, np.std],\n",
    "        \"log_return1\": [realized_volatility],\n",
    "        \"log_return2\": [realized_volatility],\n",
    "        \"wap_balance\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_book_slope\": [np.mean, np.max],\n",
    "        \"depth_imbalance1\": [np.sum, np.max, np.std],\n",
    "        \"depth_imbalance2\": [np.sum, np.max, np.std],\n",
    "        \"height_imbalance1\": [np.sum, np.max, np.std],\n",
    "        \"height_imbalance2\": [np.sum, np.max, np.std],\n",
    "        \"pressure_imbalance\": [np.sum, np.max, np.std],\n",
    "        \"total_volume\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        \"log_return1\": [realized_volatility],\n",
    "        \"log_return2\": [realized_volatility],\n",
    "        \"wap_balance\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"bid_ask_spread2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance1\": [np.sum, np.max, np.min, np.std],\n",
    "        \"order_flow_imbalance2\": [np.sum, np.max, np.min, np.std],\n",
    "        \"total_volume\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(feature_dict, seconds_in_bucket, add_suffix=False):\n",
    "        # Group by the window\n",
    "        df_feature_ = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(\n",
    "            feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature_.columns = [\"_\".join(col) for col in df_feature_.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature_ = df_feature_.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "        return df_feature_\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    windows = [0, 150, 300, 450]\n",
    "    add_suffixes = [False, True, True, True]\n",
    "    df_feature = None\n",
    "\n",
    "    for window, add_suffix in zip(windows, add_suffixes):\n",
    "        if df_feature is None:\n",
    "            df_feature = get_stats_window(feature_dict=create_feature_dict, seconds_in_bucket=window,\n",
    "                                          add_suffix=add_suffix)\n",
    "        else:\n",
    "            new_df_feature = get_stats_window(feature_dict=create_feature_dict_time, seconds_in_bucket=window,\n",
    "                                              add_suffix=add_suffix)\n",
    "            df_feature = df_feature.merge(new_df_feature, how=\"left\", left_on=\"time_id_\",\n",
    "                                          right_on=\"time_id__{}\".format(window))\n",
    "\n",
    "            # Drop unnecesary time_ids\n",
    "            df_feature.drop([\"time_id__{}\".format(window)], axis=1, inplace=True)\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature.drop([\"time_id_\"], axis=1, inplace=True)\n",
    "\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97fcef12-2b0b-477f-9e24-ad8eadc4c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # float 64 to float 32\n",
    "    float_cols = df.select_dtypes(include=[np.float64]).columns\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "\n",
    "    # int 64 to int 32\n",
    "    int_cols = df.select_dtypes(include=[np.int64]).columns\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    # rebase seconds_in_bucket\n",
    "    df[\"seconds_in_bucket\"] = df[\"seconds_in_bucket\"] - df[\"seconds_in_bucket\"].min()\n",
    "\n",
    "    # Calculate seconds gap\n",
    "    df[\"seconds_gap\"] = df.groupby([\"time_id\"])[\"seconds_in_bucket\"].apply(time_diff)\n",
    "\n",
    "    # Calculate log return\n",
    "    df[\"price_log_return\"] = df.groupby(\"time_id\")[\"price\"].apply(log_return)\n",
    "\n",
    "    # Calculate volumes\n",
    "    df[\"volumes\"] = df[\"price\"] * df[\"size\"]\n",
    "\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        \"price_log_return\": [realized_volatility],\n",
    "        \"volumes\": [np.sum, np.max, np.std],\n",
    "        \"order_count\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        \"price_log_return\": [realized_volatility],\n",
    "        \"volumes\": [np.sum, np.max, np.std],\n",
    "        \"order_count\": [np.sum],\n",
    "        \"seconds_gap\": [np.mean]\n",
    "    }\n",
    "\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(feature_dict, seconds_in_bucket, add_suffix=False):\n",
    "        # Group by the window\n",
    "        df_feature_ = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(\n",
    "            feature_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature_.columns = [\"_\".join(col) for col in df_feature_.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature_ = df_feature_.add_suffix(\"_\" + str(seconds_in_bucket))\n",
    "        return df_feature_\n",
    "\n",
    "    # Get the stats for different windows\n",
    "    windows = [0, 150, 300, 450]\n",
    "    add_suffixes = [False, True, True, True]\n",
    "    df_feature = None\n",
    "\n",
    "    for window, add_suffix in zip(windows, add_suffixes):\n",
    "        if df_feature is None:\n",
    "            df_feature = get_stats_window(feature_dict=create_feature_dict, seconds_in_bucket=window,\n",
    "                                          add_suffix=add_suffix)\n",
    "        else:\n",
    "            new_df_feature = get_stats_window(feature_dict=create_feature_dict_time, seconds_in_bucket=window,\n",
    "                                              add_suffix=add_suffix)\n",
    "            df_feature = df_feature.merge(new_df_feature, how=\"left\", left_on=\"time_id_\",\n",
    "                                          right_on=\"time_id__{}\".format(window))\n",
    "\n",
    "            # Drop unnecesary time_ids\n",
    "            df_feature.drop([\"time_id__{}\".format(window)], axis=1, inplace=True)\n",
    "\n",
    "    def tendency(price, vol):\n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff / price[1:]) * 100\n",
    "        power = np.sum(val * vol[1:])\n",
    "        return (power)\n",
    "\n",
    "    lis = []\n",
    "    for n_time_id in df[\"time_id\"].unique():\n",
    "        \n",
    "        df_id = df[df[\"time_id\"] == n_time_id]\n",
    "        \n",
    "        tendencyV = tendency(df_id[\"price\"].values, df_id[\"size\"].values)\n",
    "        energy = np.mean(df_id[\"price\"].values ** 2)\n",
    "\n",
    "        lis.append(\n",
    "            {\n",
    "                \"time_id\": n_time_id,\n",
    "                \"tendency\": tendencyV,\n",
    "                \"energy\": energy,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_feature = df_feature.merge(df_lr, how=\"left\", left_on=\"time_id_\", right_on=\"time_id\")\n",
    "\n",
    "    # Create row_id so we can merge\n",
    "    df_feature = df_feature.add_prefix(\"trade_\")\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"trade_time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature.drop([\"trade_time_id_\", \"trade_time_id\"], axis=1, inplace=True)\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2dadda6-499a-4969-bef7-2d47d8178ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = CONFIG[\"root_dir\"] + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = CONFIG[\"root_dir\"] + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = CONFIG[\"root_dir\"] + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = CONFIG[\"root_dir\"] + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = \"row_id\", how = \"left\")\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    \n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2a4b5d-63aa-4992-97b6-a3df4e43f1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   52.6s\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  4.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train[\"stock_id\"].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train=True)\n",
    "train = train.merge(train_, on=[\"row_id\"], how=\"left\")\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test[\"stock_id\"].unique()\n",
    "\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train=False)\n",
    "test = test.merge(test_, on=[\"row_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278df512-5e94-40a6-b9da-2ad0350c1346",
   "metadata": {},
   "source": [
    "# Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e56f947-264b-4a2f-83f9-4d4ec06ad913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs log columns\n",
    "abs_log_columns = [column for column in train.columns if \n",
    "                       \"order_flow_imbalance\" in column or \n",
    "                       \"order_book_slope\" in column or \n",
    "                       \"depth_imbalance\" in column or \n",
    "                       \"pressure_imbalance\" in column or\n",
    "                       \"total_volume\" in column or\n",
    "                       \"seconds_gap\" in column or\n",
    "                       \"trade_volumes\" in column or\n",
    "                       \"trade_order_count\" in column or\n",
    "                       \"trade_seconds_gap\" in column or\n",
    "                       \"trade_tendency\" in column\n",
    "                      ]\n",
    "\n",
    "# apply abs + log1p\n",
    "train[abs_log_columns] = (train[abs_log_columns].apply(np.abs)).apply(np.log1p)\n",
    "test[abs_log_columns] = (test[abs_log_columns].apply(np.abs)).apply(np.log1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829b635-1244-4389-8f98-f7a5f9114ab3",
   "metadata": {},
   "source": [
    "# Fill inf with nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43a02e48-a788-4572-a1ce-d5b554944430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8263237-9f5a-4790-b491-1cfb9636a379",
   "metadata": {},
   "source": [
    "# Agg features inside fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64e7584-8a75-4192-b88f-19ccb01653fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process agg by kmeans\n",
    "def get_kmeans_idx(n_clusters=7):\n",
    "    train_p = pd.read_csv(\"../../input/optiver-realized-volatility-prediction/train.csv\")\n",
    "    train_p = train_p.pivot(index=\"time_id\", columns=\"stock_id\", values=\"target\")\n",
    "\n",
    "    corr = train_p.corr()\n",
    "\n",
    "    ids = corr.index\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(corr.values)\n",
    "\n",
    "    kmeans_clusters = []\n",
    "    for n in range(n_clusters):\n",
    "        kmeans_clusters.append ([(x - 1) for x in ((ids + 1)*(kmeans.labels_ == n)) if x > 0])\n",
    "        \n",
    "    return kmeans_clusters\n",
    "    \n",
    "\n",
    "def agg_stat_features_by_clusters(df, n_clusters=7, function=np.nanmean, post_fix=\"_cluster_mean\"):\n",
    "\n",
    "    kmeans_clusters = get_kmeans_idx(n_clusters=n_clusters)\n",
    "\n",
    "    clusters = []\n",
    "    agg_columns = [\n",
    "        \"time_id\",\n",
    "        \"stock_id\",\n",
    "        \"log_return1_realized_volatility\",\n",
    "        \"log_return2_realized_volatility\",\n",
    "        \"order_flow_imbalance1_sum\",\n",
    "        \"order_flow_imbalance2_sum\",\n",
    "        \"order_book_slope_mean\",\n",
    "        \"depth_imbalance1_std\",\n",
    "        \"depth_imbalance2_std\",\n",
    "        \"height_imbalance1_sum\",\n",
    "        \"height_imbalance2_sum\",\n",
    "        \"pressure_imbalance_std\",\n",
    "        \"total_volume_sum\",\n",
    "        \"seconds_gap_mean\",\n",
    "        \"trade_price_log_return_realized_volatility\",\n",
    "        \"trade_volumes_sum\",\n",
    "        \"trade_order_count_sum\",\n",
    "        \"trade_seconds_gap_mean\",\n",
    "        \"trade_tendency\",\n",
    "        \"trade_energy\"\n",
    "    ]\n",
    "\n",
    "    for cluster_idx, ind in enumerate(kmeans_clusters):\n",
    "        cluster_df = df.loc[df[\"stock_id\"].isin(ind), agg_columns].groupby([\"time_id\"]).agg(function)\n",
    "        cluster_df.loc[:, \"stock_id\"] = str(cluster_idx) + post_fix\n",
    "        clusters.append(cluster_df)\n",
    "\n",
    "    clusters_df = pd.concat(clusters).reset_index()\n",
    "    # multi index (column, c1)\n",
    "    clusters_df = clusters_df.pivot(index=\"time_id\", columns=\"stock_id\")\n",
    "    # ravel multi index to list of tuple [(target, c1), ...]\n",
    "    clusters_df.columns = [\"_\".join(x) for x in clusters_df.columns.ravel()]\n",
    "    clusters_df.reset_index(inplace=True)\n",
    "\n",
    "    postfixes = [\n",
    "        \"0\" + post_fix,\n",
    "        \"1\" + post_fix,\n",
    "        \"3\" + post_fix,\n",
    "        \"4\" + post_fix,\n",
    "        \"6\" + post_fix,\n",
    "    ]\n",
    "    merge_columns = []\n",
    "    for column in agg_columns:\n",
    "        if column == \"time_id\":\n",
    "            merge_columns.append(column)\n",
    "        elif column == \"stock_id\":\n",
    "            continue\n",
    "        else:\n",
    "            for postfix in postfixes:\n",
    "                merge_columns.append(column + \"_\" + postfix)\n",
    "                \n",
    "    not_exist_columns = [column for column in merge_columns if column not in clusters_df.columns]\n",
    "    clusters_df[not_exist_columns] = 0\n",
    "    \n",
    "    df = pd.merge(df, clusters_df[merge_columns], how=\"left\", on=\"time_id\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to get group stats for the time_id\n",
    "def agg_stat_features_by_market(df, operations=None, operations_names=None):\n",
    "    def percentile(n):\n",
    "        def percentile_(x):\n",
    "            return np.percentile(x, n)\n",
    "\n",
    "        percentile_.__name__ = \"percentile_%s\" % n\n",
    "        return percentile_\n",
    "\n",
    "    if operations is None:\n",
    "        operations = [\n",
    "            np.nanmean,\n",
    "        ]\n",
    "        operations_names = [\n",
    "            \"mean\",\n",
    "        ]\n",
    "\n",
    "    # Get realized volatility columns\n",
    "    vol_cols = [\n",
    "        \"log_return1_realized_volatility\",\n",
    "        \"log_return1_realized_volatility_150\",\n",
    "        \"log_return1_realized_volatility_300\",\n",
    "        \"log_return1_realized_volatility_450\",\n",
    "    ]\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby([\"stock_id\"])[vol_cols].agg(operations).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = [\"_\".join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix(\"_\" + \"stock\")\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby([\"time_id\"])[vol_cols].agg(operations).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = [\"_\".join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix(\"_\" + \"time\")\n",
    "\n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how=\"left\", left_on=[\"stock_id\"], right_on=[\"stock_id__stock\"])\n",
    "    df.drop(\"stock_id__stock\", axis=1, inplace=True)\n",
    "\n",
    "    df = df.merge(df_time_id, how=\"left\", left_on=[\"time_id\"], right_on=[\"time_id__time\"])\n",
    "    df.drop(\"time_id__time\", axis=1, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a95433-524b-4d66-a6d2-3208e3449ed2",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "398a9da6-4930-4da3-8bc6-5a9b210b062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        \n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "    \n",
    "\n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8775544d-92b2-44b3-9f66-8b2ec8422748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test):\n",
    "    \n",
    "    # label encoder\n",
    "    cat_columns = [\"stock_id\"]\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder = label_encoder.fit(train[cat_columns].values)\n",
    "    train[cat_columns] = label_encoder.transform(train[cat_columns].values)\n",
    "    dump(label_encoder, os.path.join(CONFIG[\"ckpt_path\"], \"label_encoder.bin\"), compress=True)\n",
    "    test[cat_columns] = label_encoder.transform(test[cat_columns].values)\n",
    "    cat_dims = [len(label_encoder.classes_)]\n",
    "    \n",
    "    # scaler\n",
    "    # scaler = QuantileTransformer(n_quantiles=2000, random_state=2021)\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Split features and target\n",
    "    x = train.drop([\"row_id\", \"target\"], axis=1)\n",
    "    y = train[\"target\"]\n",
    "    \n",
    "    # x_test with train feature\n",
    "    x_test = test.drop(\"row_id\", axis=1)\n",
    "    x_test = agg_stat_features_by_market(x_test)\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "    x_test = agg_stat_features_by_clusters(x_test, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "\n",
    "    # define normalize columns\n",
    "    except_columns = [\"stock_id\", \"time_id\", \"target\", \"row_id\"]\n",
    "    normalized_columns = [column for column in x_test.columns if column not in except_columns]\n",
    "    x_test.drop(\"time_id\", axis=1, inplace=True)\n",
    "    \n",
    "    # Process categorical features and get params dict\n",
    "    cat_idxs = [i for i, f in enumerate(x_test.columns.tolist()) if f in cat_columns]\n",
    "    \n",
    "    params = dict(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        cat_emb_dim=1,\n",
    "        n_d=16,\n",
    "        n_a=16,\n",
    "        n_steps=2,\n",
    "        gamma=2,\n",
    "        n_independent=2,\n",
    "        n_shared=2,\n",
    "        lambda_sparse=0,\n",
    "        optimizer_fn=Adam,\n",
    "        optimizer_params=dict(lr = (2e-2)),\n",
    "        mask_type=\"entmax\",\n",
    "        scheduler_params=dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
    "        scheduler_fn=CosineAnnealingWarmRestarts,\n",
    "        seed=42,\n",
    "        verbose=10\n",
    "    )\n",
    "    \n",
    "    # Create out of folds array\n",
    "    oof_predictions = np.zeros(x.shape[0])\n",
    "    \n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(x_test.shape[0])\n",
    "    \n",
    "    # Statistics\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances[\"feature\"] = x_test.columns.tolist()\n",
    "    stats = pd.DataFrame()\n",
    "    explain_matrices = []\n",
    "    masks_ =[]\n",
    "    \n",
    "    # Create a KFold object\n",
    "    kfold = GroupKFold(n_splits=CONFIG[\"n_splits\"])\n",
    "    \n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x, groups=x[\"time_id\"])):\n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        x_train = x.iloc[trn_ind]\n",
    "        x_train = agg_stat_features_by_market(x_train)\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "        x_train = agg_stat_features_by_clusters(x_train, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "        x_train.drop(\"time_id\", axis=1, inplace=True)\n",
    "        \n",
    "        scaler = scaler.fit(x_train[normalized_columns])\n",
    "        dump(scaler, os.path.join(CONFIG[\"ckpt_path\"], \"tabnet_std_scaler_fold_{}.bin\".format(fold + 1)), compress=True)\n",
    "        x_train = x_train.fillna(x_train.mean())\n",
    "        x_train[normalized_columns] = scaler.transform(x_train[normalized_columns])\n",
    "        # x_train = x_train.fillna(0)\n",
    "        \n",
    "        x_val = x.iloc[val_ind]\n",
    "        x_val = agg_stat_features_by_market(x_val)\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmean, post_fix=\"_cluster_mean\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmax, post_fix=\"_cluster_max\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanmin, post_fix=\"_cluster_min\")\n",
    "        x_val = agg_stat_features_by_clusters(x_val, n_clusters=CONFIG[\"n_clusters\"], function=np.nanstd, post_fix=\"_cluster_std\")\n",
    "        x_val.drop(\"time_id\", axis=1, inplace=True)\n",
    "        \n",
    "        x_val[normalized_columns] = scaler.transform(x_val[normalized_columns])\n",
    "        x_val = x_val.fillna(0)\n",
    "        \n",
    "        y_train, y_val = y.iloc[trn_ind].values.reshape(-1, 1), y.iloc[val_ind].values.reshape(-1, 1)\n",
    "        \n",
    "        # Train\n",
    "        clf =  TabNetRegressor(**params)\n",
    "        \n",
    "        if CONFIG[\"pretrained\"] and os.path.exists(os.path.join(CONFIG[\"ckpt_path\"], \"tabnet_fold{}.zip\".format(fold + 1))):\n",
    "            clf.load_model(os.path.join(CONFIG[\"ckpt_path\"], \"tabnet_fold{}.zip\".format(fold + 1)))\n",
    "        else:\n",
    "            clf.fit(\n",
    "                  x_train.values, y_train,\n",
    "                  eval_set=[(x_val.values, y_val)],\n",
    "                  max_epochs=200,\n",
    "                  patience=50,\n",
    "                  batch_size=1024*20, \n",
    "                  virtual_batch_size=128*20,\n",
    "                  num_workers=0,\n",
    "                  drop_last=False,\n",
    "                  eval_metric=[RMSPE],\n",
    "                  loss_fn=RMSPELoss\n",
    "              )\n",
    "\n",
    "            # save model\n",
    "            saved_filepath = clf.save_model(os.path.join(CONFIG[\"ckpt_path\"], \"tabnet_fold{}\".format(fold + 1)))\n",
    "        \n",
    "        # save statistics\n",
    "        explain_matrix, masks = clf.explain(x_val.values)\n",
    "        explain_matrices.append(explain_matrix)\n",
    "        masks_.append(masks[0])\n",
    "        masks_.append(masks[1])\n",
    "        \n",
    "        # save oof and test predictions\n",
    "        oof_predictions[val_ind] = clf.predict(x_val.values).flatten()\n",
    "        x_test_ = x_test.copy()\n",
    "        \n",
    "        x_test_[normalized_columns] = scaler.transform(x_test_[normalized_columns])\n",
    "        x_test_ = x_test_.fillna(0)\n",
    "        \n",
    "        test_predictions += clf.predict(x_test_.values).flatten() / CONFIG[\"n_splits\"]\n",
    "        \n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(\"Our out of folds RMSPE is {}\".format(rmspe_score))\n",
    "    \n",
    "    # Return test predictions\n",
    "    return test_predictions, stats, feature_importances, explain_matrices, masks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15fb8ffa-a102-402c-82b6-aedf895d02f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling dataset\n",
      "Training fold 1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 121.62818| val_0_rmspe: 21.85942|  0:00:09s\n",
      "epoch 10 | loss: 0.36708 | val_0_rmspe: 0.30892 |  0:01:32s\n",
      "epoch 20 | loss: 0.30221 | val_0_rmspe: 0.3151  |  0:02:55s\n",
      "epoch 30 | loss: 0.28016 | val_0_rmspe: 0.25657 |  0:04:17s\n",
      "epoch 40 | loss: 0.25096 | val_0_rmspe: 0.25064 |  0:05:40s\n",
      "epoch 50 | loss: 0.23917 | val_0_rmspe: 0.25072 |  0:07:02s\n",
      "epoch 60 | loss: 0.24312 | val_0_rmspe: 0.2627  |  0:08:24s\n",
      "epoch 70 | loss: 0.22007 | val_0_rmspe: 0.23015 |  0:09:46s\n",
      "epoch 80 | loss: 0.2309  | val_0_rmspe: 0.2539  |  0:11:08s\n",
      "epoch 90 | loss: 0.20857 | val_0_rmspe: 0.22997 |  0:12:30s\n",
      "epoch 100| loss: 0.20208 | val_0_rmspe: 0.22648 |  0:13:52s\n",
      "epoch 110| loss: 0.19781 | val_0_rmspe: 0.2323  |  0:15:15s\n",
      "epoch 120| loss: 0.19643 | val_0_rmspe: 0.23158 |  0:16:37s\n",
      "epoch 130| loss: 0.19632 | val_0_rmspe: 0.23284 |  0:17:59s\n",
      "epoch 140| loss: 0.19036 | val_0_rmspe: 0.23329 |  0:19:21s\n",
      "\n",
      "Early stopping occurred at epoch 149 with best_epoch = 99 and best_val_0_rmspe = 0.2264\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ../../ckpts/tabnet_fold1.zip\n",
      "Training fold 2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 183.41756| val_0_rmspe: 45.50032|  0:00:09s\n",
      "epoch 10 | loss: 0.39734 | val_0_rmspe: 0.48336 |  0:01:48s\n",
      "epoch 20 | loss: 0.54691 | val_0_rmspe: 0.37925 |  0:03:26s\n",
      "epoch 30 | loss: 0.31078 | val_0_rmspe: 0.30519 |  0:05:04s\n",
      "epoch 40 | loss: 0.25569 | val_0_rmspe: 0.24768 |  0:06:43s\n",
      "epoch 50 | loss: 0.22288 | val_0_rmspe: 0.27254 |  0:08:21s\n",
      "epoch 60 | loss: 0.22453 | val_0_rmspe: 0.23902 |  0:09:59s\n",
      "epoch 70 | loss: 0.21411 | val_0_rmspe: 0.23267 |  0:11:38s\n",
      "epoch 80 | loss: 0.20993 | val_0_rmspe: 0.23069 |  0:13:16s\n",
      "epoch 90 | loss: 0.21001 | val_0_rmspe: 0.24425 |  0:14:54s\n",
      "epoch 100| loss: 0.19957 | val_0_rmspe: 0.23417 |  0:16:32s\n",
      "epoch 110| loss: 0.19859 | val_0_rmspe: 0.2408  |  0:18:10s\n",
      "epoch 120| loss: 0.19654 | val_0_rmspe: 0.23732 |  0:19:48s\n",
      "epoch 130| loss: 0.19244 | val_0_rmspe: 0.23779 |  0:21:26s\n",
      "epoch 140| loss: 0.18857 | val_0_rmspe: 0.23938 |  0:23:04s\n",
      "epoch 150| loss: 0.18668 | val_0_rmspe: 0.24153 |  0:24:42s\n",
      "\n",
      "Early stopping occurred at epoch 156 with best_epoch = 106 and best_val_0_rmspe = 0.22921\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ../../ckpts/tabnet_fold2.zip\n",
      "Training fold 3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 208.9794| val_0_rmspe: 16.90974|  0:00:08s\n",
      "epoch 10 | loss: 0.41564 | val_0_rmspe: 0.40491 |  0:01:31s\n",
      "epoch 20 | loss: 0.31795 | val_0_rmspe: 0.30169 |  0:02:54s\n",
      "epoch 30 | loss: 0.2684  | val_0_rmspe: 0.23458 |  0:04:17s\n",
      "epoch 40 | loss: 0.22695 | val_0_rmspe: 0.2373  |  0:05:41s\n",
      "epoch 50 | loss: 0.22583 | val_0_rmspe: 0.23901 |  0:07:04s\n",
      "epoch 60 | loss: 0.22019 | val_0_rmspe: 0.22465 |  0:08:27s\n",
      "epoch 70 | loss: 0.24035 | val_0_rmspe: 0.23808 |  0:09:50s\n",
      "epoch 80 | loss: 0.21611 | val_0_rmspe: 0.22189 |  0:11:13s\n",
      "epoch 90 | loss: 0.20561 | val_0_rmspe: 0.22172 |  0:12:36s\n",
      "epoch 100| loss: 0.20114 | val_0_rmspe: 0.22208 |  0:13:59s\n",
      "epoch 110| loss: 0.20069 | val_0_rmspe: 0.22246 |  0:15:22s\n",
      "epoch 120| loss: 0.19484 | val_0_rmspe: 0.23131 |  0:16:45s\n",
      "epoch 130| loss: 0.19243 | val_0_rmspe: 0.23161 |  0:18:09s\n",
      "\n",
      "Early stopping occurred at epoch 139 with best_epoch = 89 and best_val_0_rmspe = 0.22006\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ../../ckpts/tabnet_fold3.zip\n",
      "Training fold 4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 141.46729| val_0_rmspe: 30.15196|  0:00:08s\n",
      "epoch 10 | loss: 0.5893  | val_0_rmspe: 0.48136 |  0:01:31s\n",
      "epoch 20 | loss: 0.29845 | val_0_rmspe: 0.47484 |  0:02:54s\n",
      "epoch 30 | loss: 0.31521 | val_0_rmspe: 0.2629  |  0:04:18s\n",
      "epoch 40 | loss: 0.29503 | val_0_rmspe: 0.28291 |  0:05:41s\n",
      "epoch 50 | loss: 0.22251 | val_0_rmspe: 0.22761 |  0:07:04s\n",
      "epoch 60 | loss: 0.2374  | val_0_rmspe: 0.21861 |  0:08:27s\n",
      "epoch 70 | loss: 0.21773 | val_0_rmspe: 0.22084 |  0:09:50s\n",
      "epoch 80 | loss: 0.2394  | val_0_rmspe: 0.23446 |  0:11:14s\n",
      "epoch 90 | loss: 0.20513 | val_0_rmspe: 0.22032 |  0:12:37s\n",
      "epoch 100| loss: 0.20074 | val_0_rmspe: 0.22123 |  0:14:00s\n",
      "epoch 110| loss: 0.19501 | val_0_rmspe: 0.22418 |  0:15:23s\n",
      "\n",
      "Early stopping occurred at epoch 110 with best_epoch = 60 and best_val_0_rmspe = 0.21861\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ../../ckpts/tabnet_fold4.zip\n",
      "Training fold 5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 134.8847| val_0_rmspe: 17.23435|  0:00:08s\n",
      "epoch 10 | loss: 0.45755 | val_0_rmspe: 0.37149 |  0:01:31s\n",
      "epoch 20 | loss: 0.39008 | val_0_rmspe: 0.6569  |  0:02:55s\n",
      "epoch 30 | loss: 0.25139 | val_0_rmspe: 0.25403 |  0:04:18s\n",
      "epoch 40 | loss: 0.25736 | val_0_rmspe: 0.24205 |  0:05:41s\n",
      "epoch 50 | loss: 0.25995 | val_0_rmspe: 0.23796 |  0:07:04s\n",
      "epoch 60 | loss: 0.22213 | val_0_rmspe: 0.21923 |  0:08:27s\n",
      "epoch 70 | loss: 0.2131  | val_0_rmspe: 0.26633 |  0:09:50s\n",
      "epoch 80 | loss: 0.25744 | val_0_rmspe: 0.22293 |  0:11:13s\n",
      "epoch 90 | loss: 0.19953 | val_0_rmspe: 0.22262 |  0:12:37s\n",
      "epoch 100| loss: 0.19656 | val_0_rmspe: 0.22801 |  0:14:00s\n",
      "epoch 110| loss: 0.19224 | val_0_rmspe: 0.22953 |  0:15:23s\n",
      "\n",
      "Early stopping occurred at epoch 115 with best_epoch = 65 and best_val_0_rmspe = 0.21636\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at ../../ckpts/tabnet_fold5.zip\n",
      "Our out of folds RMSPE is 0.22218131961691437\n"
     ]
    }
   ],
   "source": [
    "# Traing and evaluate\n",
    "if CONFIG[\"shuffle\"]:\n",
    "    print(\"shuffling dataset\")\n",
    "    train = train.sample(frac=1, random_state=CONFIG[\"shuffle_seed\"]).reset_index(drop=True)\n",
    "test_predictions, stats, feature_importances, explain_matrices, masks_ = train_and_evaluate(train, test)\n",
    "\n",
    "# Save test predictions\n",
    "# test[\"target\"] = test_predictions\n",
    "# test[[\"row_id\", \"target\"]].to_csv(\"submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae80f9-0185-40c6-9235-11c08bb7acdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
